{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need fully deterministic results between runs, set the following environment value prior to launching jupyter.\n",
    "# See comment in sherlock.features.paragraph_vectors.infer_paragraph_embeddings_features for more info.\n",
    "%env PYTHONHASHSEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features, retrain Sherlock and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below first downloads the data (roughly 700K samples), then extract features from the raw data values. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pyfunctional==1.4.3\n",
    "-e .\n",
    "gdown==4.3.0\n",
    "nltk==3.4.5\n",
    "gensim==3.8.0\n",
    "'''\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --user numpy\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Started at {datetime.now()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "This will download the raw values and preprocessed files, the corresponding labels as well as a few other supporting files:\n",
    "- `download_data()` will download 3.6GB of data for preprocessing and model training into the `data/` directory.\n",
    "- `prepare_feature_extraction()` will download +/- 800 MB of data into the `features/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy'):\n",
    "    raise SystemExit(\n",
    "        \"\"\"\n",
    "        Trained paragraph vectors do not exist,\n",
    "        please run the '03-retrain-paragraph-vector-features' notebook before continuing\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in raw data\n",
    "You can skip this step if you want to use a preprocessed data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report memory usage (can be slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "The input data is assumed to be a dataframe of column values stored as stringed lists \"['hello', 'goodbye', 'hi']\", stored in a parquet file.\n",
    "\n",
    "`TODO`: ideally we pickle the dataframe with value lists to avoid parsing these strings into lists again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Features will be output to the following files\n",
    "X_test_filename_csv = f'../data/data/processed/test_{timestr}.csv'\n",
    "X_train_filename_csv = f'../data/data/processed/train_{timestr}.csv'\n",
    "X_validation_filename_csv = f'../data/data/processed/validation_{timestr}.csv'\n",
    "\n",
    "X_orm_filename_csv = f'../data/data/processed/ormtrain_{timestr}.csv'\n",
    "\n",
    "## use preprocessed files \n",
    "X_test_filename_csv = f'../data/data/processed/test_20220323-163452.csv'\n",
    "X_train_filename_csv = f'../data/data/processed/train_20220323-163452.csv'\n",
    "X_validation_filename_csv = f'../data/data/processed/validation_20220323-163452.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default process creation mode is changed in Python 3.8 to 'spawn' which causes \"name not defined\" type errors. Force fork 'mode' for now (this only needs to be called once).\n",
    "# https://bugs.python.org/issue39931\n",
    "#mp.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = load_parquet_values(\"../data/data/raw/test_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_test_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_features_to_csv(X_test_filename_csv, values)\n",
    "\n",
    "X_test_filename_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = load_parquet_values(\"../data/data/raw/train_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_train_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = load_parquet_values(\"../data/data/raw/val_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_validation_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Locally Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "X_test = pd.read_csv(X_test_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_csv(X_train_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_validation = pd.read_csv(X_validation_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN values with feature means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_columns_means = pd.DataFrame(X_train.mean()).transpose()\n",
    "\n",
    "print(f'Transpose process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_validation.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_test.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_orm.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "\n",
    "train_columns_means=None\n",
    "\n",
    "print(f'FillNA process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train.to_parquet('../data/data/processed/train.parquet', engine='pyarrow', compression='snappy')\n",
    "X_validation.to_parquet('../data/data/processed/validation.parquet', engine='pyarrow', compression='snappy')\n",
    "X_test.to_parquet('../data/data/processed/test.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "print(f'Save parquet process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Completed at {datetime.now()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orm.to_parquet('../data/data/processed/orm.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env5)",
   "language": "python",
   "name": "env5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
