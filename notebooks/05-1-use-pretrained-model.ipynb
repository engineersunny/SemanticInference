{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "2023-03-20 17:16:20.920162: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 17:16:20.920279: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 17:16:20.920291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sherlock.deploy import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d556ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "  \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SubModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    training: bool = True\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # batchnormalisation - https://github.com/google/flax/issues/932\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        x = nn.relu(nn.Dense(self.features[0])(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "                \n",
    "        x = nn.relu(nn.Dense(self.features[1])(x)) \n",
    "        \n",
    "        # todo: add \n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    num_classes: int = 78\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x1, x2, x3, x4):\n",
    "       \n",
    "        # [1] define shape        \n",
    "        y1 = SubModel([300, 300], name='char_model')(x1)      \n",
    "        y2 = SubModel([200, 200], name='word_model')(x2)\n",
    "        y3 = SubModel([400, 400], name='par_model')(x3)\n",
    "        y4 = RestModel([27], name='rest_model')(x4)\n",
    "                      \n",
    "        # [2] concat submodels    \n",
    "        x = jnp.concatenate((y1, y2, y3, y4), axis=-1)\n",
    "        \n",
    "        print(\"check mainmodel shape\")\n",
    "        print(np.shape(x))\n",
    "        \n",
    "        # batchnormalisation\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        # dense 1\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "        \n",
    "        # dense 2\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dense w/ softmax - todo: check\n",
    "        x = nn.softmax(nn.Dense(self.feature_size)(x), axis=-1)\n",
    "        \n",
    "        return nn.Dense(self.num_classes)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8223860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state \n",
    "\n",
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "def create_train_state(rng):\n",
    "  mainmodel = MainModel()\n",
    "  params = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27)))\n",
    "  tx=optax.adam(learning_rate=learning_rate)\n",
    "  state = train_state.TrainState.create(apply_fn=mainmodel.apply,\n",
    "                                        params=params,\n",
    "                                        tx=tx)\n",
    "  return state \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db9275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "## RESTORE\n",
    "# file - byte -> TrainState\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "with open(\"flax_model_MAR.msgpack\", mode='rb') as file:\n",
    "    read_data = file.read()\n",
    "\n",
    "new_state = create_train_state(init_rng)    \n",
    "restored_state = flax.serialization.from_bytes(new_state, read_data)  ##TrainState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1601d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd14f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo raw data -> featurization\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/inputX.parquet')\n",
    "# y_validation = pd.read_parquet('../data/data/raw/inputY_pq.parquet').values.flatten()\n",
    "y_val = pd.read_parquet('../data/data/raw/inputY_pq.parquet')\n",
    "y_val = y_val.values.flatten() #to ndarray\n",
    "\n",
    "\n",
    "# y_val = y_val.applymap(str)\n",
    "y_validation = np.array([x.lower() for x in y_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fde7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char = X_test[feature_cols[\"char\"]]\n",
    "X_word = X_test[feature_cols[\"word\"]]\n",
    "X_par = X_test[feature_cols[\"par\"]]\n",
    "X_rest = X_test[feature_cols[\"rest\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aafc0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_v_1 = jnp.array(pd.DataFrame(X_char).to_numpy())\n",
    "j_v_2 = jnp.array(pd.DataFrame(X_word).to_numpy())\n",
    "j_v_3 = jnp.array(pd.DataFrame(X_par).to_numpy())\n",
    "j_v_4 = jnp.array(pd.DataFrame(X_rest).to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f849173b",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b3da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 17:21:13.199583: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,300]{1,0} custom-call(f32[4,960]{1,0} %Arg_0.1, f32[960,300]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:13.366751: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,300]{1,0} custom-call(f32[4,300]{1,0} %Arg_0.1, f32[300,300]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=32}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:13.598403: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,200]{1,0} custom-call(f32[4,201]{1,0} %Arg_0.1, f32[201,200]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:13.779518: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,200]{1,0} custom-call(f32[4,200]{1,0} %Arg_0.1, f32[200,200]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=32}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:14.003061: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,400]{1,0} custom-call(f32[4,400]{1,0} %Arg_0.1, f32[400,400]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(4, 927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 17:21:14.652855: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,500]{1,0} custom-call(f32[4,927]{1,0} %Arg_0.1, f32[927,500]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=65}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:14.848377: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,500]{1,0} custom-call(f32[4,500]{1,0} %Arg_0.1, f32[500,500]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=71}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-20 17:21:15.375277: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[4,78]{1,0} custom-call(f32[4,500]{1,0} %Arg_0.1, f32[500,78]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_364921/3202350427.py\" source_line=76}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mainmodel = MainModel()\n",
    "y_pred_sv = mainmodel.apply(new_state.params, j_v_1, j_v_2, j_v_3, j_v_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb61e58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['affiliate', 'address', 'address', 'family'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes = helpers._proba_to_classes(y_pred_sv, \"sherlock\")\n",
    "\n",
    "y_pred_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eca1f280",
   "metadata": {},
   "source": [
    "Score - additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523579af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_validation, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d038d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area' 'collection' 'team name' 'credit']\n",
      "['affiliate' 'address' 'address' 'family']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(y_validation)\n",
    "print(y_pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherlock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
