{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training set and train paragraph vectors\n",
    "Note: the paragraph vector model has been trained and is downloaded in the `prepare_feature_extraction()` function.\n",
    "\n",
    "Retraining is therefore not needed, but optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=13\n"
     ]
    }
   ],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "# If you need fully deterministic results between runs, set the following environment value prior to launching jupyter.\n",
    "# See comment in sherlock.features.paragraph_vectors.infer_paragraph_embeddings_features for more info.\n",
    "%env PYTHONHASHSEED =13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-09-13 16:04:49.493998\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.features.paragraph_vectors import (\n",
    "    initialise_nltk,\n",
    "    tagcol_paragraph_embeddings_features,\n",
    "    train_paragraph_embeddings_features\n",
    ")\n",
    "from sherlock.features.preprocessing import convert_string_lists_to_lists\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "\n",
    "print(f'Started at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read in raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the raw data into ../data/data/.\n",
      "Data was downloaded.\n"
     ]
    }
   ],
   "source": [
    "helpers.download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = pd.read_parquet('../data/data/raw/train_values.parquet')\n",
    "train_labels = pd.read_parquet('../data/data/raw/train_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   values\n",
      "55030            ['Global', 'United States', 'Australia']\n",
      "167000  ['Fiction, Adult - Non-Floating', 'Fiction, Ad...\n",
      "638282  ['', '', 'University of Puerto Rico - Rio Pied...\n",
      "232298  ['Laughology', 'MTV', 'With Intent to Kill', '...\n",
      "316158  ['Mare', 'Gelding', 'Gelding', 'Gelding', 'Gel...\n",
      "467776  ['V.P., General Counsel & Sec.', 'V.P., Genera...\n",
      "149640  ['GAJA', 'OREG', 'UCS', 'WCM', 'SLAM', 'ARIZ',...\n",
      "23556   ['Applied Mathematics, University of Notre Dam...\n",
      "263802  ['wakeup time in seconds for pbid to run its c...\n",
      "476881     [35.0, 4.0, 52.0, 0.0, 30.0, 64.0, 84.0, None]\n",
      "               type\n",
      "55030          area\n",
      "167000   collection\n",
      "638282    team Name\n",
      "232298       credit\n",
      "316158       gender\n",
      "467776     position\n",
      "149640         club\n",
      "23556   affiliation\n",
      "263802  description\n",
      "476881     position\n"
     ]
    }
   ],
   "source": [
    "print(train_samples.head(10))\n",
    "print(train_labels.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 412059/412059 [00:37<00:00, 11036.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_samples_converted, y_train = convert_string_lists_to_lists(train_samples, train_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412059,)\n",
      "['Global', 'United States', 'Australia'] area\n",
      "['Norwegian Cod Liver Oil Cherry', 'Norwegian Cod Liver Oil Mint'] product\n"
     ]
    }
   ],
   "source": [
    "print(train_samples_converted.shape)\n",
    "\n",
    "print(train_samples_converted.iloc[0], y_train[0])\n",
    "print(train_samples_converted.iloc[412058], y_train[412058])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55030                    [Global, United States, Australia]\n",
      "167000    [Fiction, Adult - Non-Floating, Fiction, Adult...\n",
      "638282    [, , University of Puerto Rico - Rio Piedras, ...\n",
      "232298    [Laughology, MTV, With Intent to Kill, Comedy ...\n",
      "316158    [Mare, Gelding, Gelding, Gelding, Gelding, Mar...\n",
      "467776    [V.P., General Counsel & Sec., V.P., General C...\n",
      "149640    [GAJA, OREG, UCS, WCM, SLAM, ARIZ, NEM, VEN, M...\n",
      "23556     [Applied Mathematics, University of Notre Dame...\n",
      "263802    [wakeup time in seconds for pbid to run its ch...\n",
      "476881           [35.0, 4.0, 52.0, 0.0, 30.0, 64.0, 84.0, ]\n",
      "Name: values, dtype: object\n",
      "['area', 'collection', 'team Name', 'credit', 'gender', 'position', 'club', 'affiliation', 'description', 'position']\n"
     ]
    }
   ],
   "source": [
    "#print(train_samples_converted.head)\n",
    "\n",
    "print(train_samples_converted.head(10))\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sunny/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised NLTK, process took 0:00:00.473426 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sunny/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: <class 'pandas.core.series.Series'>, length=412059\n",
      "Labels:  <class 'pandas.core.frame.DataFrame'>, length=412059\n",
      "Labels:  <class 'numpy.ndarray'>, length=412059\n"
     ]
    }
   ],
   "source": [
    "samples = train_samples_converted.dropna()\n",
    "print(f'Samples: {type(samples)}, length={len(samples)}')\n",
    "\n",
    "train_labels = train_labels.dropna()\n",
    "\n",
    "print(f'Labels:  {type(train_labels)}, length={len(train_labels)}')\n",
    "#print(train_labels) #df\n",
    "\n",
    "labels = train_labels.values.flatten()\n",
    "print(f'Labels:  {type(labels)}, length={len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55030                    [Global, United States, Australia]\n",
      "167000    [Fiction, Adult - Non-Floating, Fiction, Adult...\n",
      "638282    [, , University of Puerto Rico - Rio Piedras, ...\n",
      "232298    [Laughology, MTV, With Intent to Kill, Comedy ...\n",
      "316158    [Mare, Gelding, Gelding, Gelding, Gelding, Mar...\n",
      "467776    [V.P., General Counsel & Sec., V.P., General C...\n",
      "149640    [GAJA, OREG, UCS, WCM, SLAM, ARIZ, NEM, VEN, M...\n",
      "23556     [Applied Mathematics, University of Notre Dame...\n",
      "263802    [wakeup time in seconds for pbid to run its ch...\n",
      "476881           [35.0, 4.0, 52.0, 0.0, 30.0, 64.0, 84.0, ]\n",
      "Name: values, dtype: object\n",
      "['area' 'collection' 'team Name' 'credit' 'gender' 'position' 'club'\n",
      " 'affiliation' 'description' 'position']\n"
     ]
    }
   ],
   "source": [
    "print(samples.head(10))\n",
    "print(labels[:10])\n",
    "\n",
    "samples = samples.head(10)\n",
    "labels = labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging columns\n",
      "Tagged Columns Doc2Vec Model, process took 0:00:00.011392 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "print('Tagging columns')\n",
    "cols = tagcol_paragraph_embeddings_features(samples, labels)\n",
    "\n",
    "#print(cols)\n",
    "print(f'Tagged Columns Doc2Vec Model, process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragraph_vectors.py\n",
    "import random; import nltk; from nltk.corpus import stopwords; from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "STOPWORDS_ENGLISH = stopwords.words(\"english\")\n",
    "\n",
    "def tokenise(values):\n",
    "    joined = \" \".join(s for s in values if len(s) >= 2)\n",
    "\n",
    "    # stopwords need apostrophe\n",
    "    filtered = \"\".join(\n",
    "        e for e in joined if e.isalnum() or e.isspace() or e == \"'\"\n",
    "    ).lower()\n",
    "\n",
    "    return [\n",
    "        word\n",
    "        for word in nltk.word_tokenize(filtered)\n",
    "        if len(word) >= 2 and word not in STOPWORDS_ENGLISH\n",
    "    ]\n",
    "\n",
    "def tagcol_paragraph_embeddings_features_nb(train_data: pd.Series, train_labels: list):\n",
    "    random.seed(13)\n",
    "\n",
    "    columns = []\n",
    "\n",
    "    for i, col in enumerate(train_data):\n",
    "        label = train_labels[i]\n",
    "        values = random.sample(col, min(1000, len(col)))\n",
    "\n",
    "        if len(values) > 0:\n",
    "            values = list(map(lambda s: \"\" if s is None else str(s), values))\n",
    "\n",
    "        tokens = tokenise(values)\n",
    "\n",
    "        columns.append(TaggedDocument(tokens, label))\n",
    "\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['united', 'states', 'australia', 'global'], area)\n",
      "['united', 'states', 'australia', 'global']\n",
      "area \n",
      "\n",
      "Tagged Columns Doc2Vec Model, process took 0:00:00.003630 seconds. \n",
      "\n",
      "[TaggedDocument(words=['united', 'states', 'australia', 'global'], tags='area'), TaggedDocument(words=['fiction', 'adult', 'fiction', 'adult', 'fiction', 'adult', 'nonfloating', 'fiction', 'adult', 'fiction', 'adult'], tags='collection'), TaggedDocument(words=['university', 'puerto', 'rico', 'rio', 'piedras', 'parck', 'place', 'dealerships', 'sun', 'university', 'puerto', 'ricorio', 'piedras', 'university', 'puerto', 'ricorio', 'piedras', 'park', 'place', 'dealerships', 'university', 'puerto', 'rico', 'rio', 'piedras', 'park', 'place', 'dealerships', 'university', 'puerto', 'ricorio', 'piedras', 'parck', 'place', 'dealerships', 'sun', 'park', 'place', 'dealerships', 'park', 'place', 'dealerships', 'park', 'place', 'dealerships', 'park', 'place', 'dealerships', 'carolina', 'tri', 'university', 'puerto', 'ricorio', 'piedras', 'park', 'place', 'dealerships', 'carolina', 'tri', 'park', 'place', 'dealerships', 'park', 'place', 'dealerships', 'university', 'puerto', 'rico', 'rio', 'piedras', 'university', 'puerto', 'ricorio', 'piedras', 'dallas', 'racing', 'university', 'puerto', 'ricorio', 'piedras', 'parck', 'place', 'dealerships', 'sun'], tags='team Name'), TaggedDocument(words=['intent', 'kill', 'laughology', 'mtv', 'comedy', 'hideaway', 'comedy', 'time', 'radio'], tags='credit'), TaggedDocument(words=['gelding', 'mare', 'mare', 'colt', 'mare', 'mare', 'gelding', 'mare', 'mare', 'gelding', 'gelding', 'gelding', 'gelding', 'gelding', 'mare', 'gelding', 'mare', 'gelding', 'mare', 'mare', 'filly', 'gelding', 'filly', 'gelding', 'mare', 'mare', 'mare', 'mare', 'gelding', 'gelding', 'mare', 'gelding', 'mare', 'mare', 'gelding', 'mare', 'mare', 'gelding', 'gelding', 'gelding', 'gelding', 'mare', 'mare', 'mare', 'filly', 'gelding', 'gelding', 'gelding', 'mare', 'mare'], tags='gender'), TaggedDocument(words=['vp', 'general', 'counsel', 'sec', 'vp', 'general', 'counsel', 'sec', 'vp', 'general', 'counsel', 'sec'], tags='position'), TaggedDocument(words=['vmst', 'wcm', 'ven', 'oreg', 'slam', 'gaja', 'ariz', 'nem', 'ucs', 'mcm'], tags='club'), TaggedDocument(words=['developmental', 'biology', 'stowers', 'institute', 'medical', 'research', 'developmental', 'biology', 'program', 'memorial', 'sloankettering', 'cancer', 'center', 'loci', 'university', 'wisconsin', 'madison', 'engineering', 'auckland', 'bioengineering', 'institute', 'cell', 'biology', 'duke', 'university', 'department', 'mathematics', 'florida', 'state', 'university', 'mathematics', 'statistics', 'smith', 'college', 'systems', 'computational', 'biology', 'lane', 'center', 'computational', 'biology', 'carnegiemellon', 'university', 'electrical', 'computer', 'engineering', 'university', 'houston', 'division', 'mathematics', 'university', 'dundee', 'oncology', 'oxford', 'university', 'mathematics', 'university', 'exeter', 'electrical', 'computer', 'engineering', 'university', 'california', 'santa', 'barbara', 'mathematics', 'university', 'california', 'irvine', 'cell', 'regenerative', 'biology', 'university', 'wisconsin', 'applied', 'mathematics', 'university', 'notre', 'dame', 'tumor', 'cell', 'biology', 'lab', 'london', 'research', 'institute', 'mathematics', 'department', 'university', 'british', 'columbia', 'developmental', 'biology', 'genetics', 'stanford', 'university', 'biology', 'ecology', 'university', 'maine', 'department', 'computer', 'science', 'molecular', 'physiology', 'biophysics', 'baylor', 'college', 'medicine', 'college', 'life', 'sciences', 'university', 'dundee', 'pathology', 'laboratory', 'medicine', 'university', 'california', 'davis', 'developmental', 'biology', 'sloankettering', 'mathematics', 'statistics', 'boston', 'university', 'lee', 'moffitt', 'cancer', 'center', 'research', 'institute', 'lee', 'moffitt', 'cancer', 'center', 'research', 'institute', 'systems', 'computational', 'biology', 'albert', 'einstein', 'college', 'medicine', 'centre', 'mathematical', 'biology', 'mathematical', 'institute', 'oncology', 'oxford', 'university', 'department', 'cell', 'biology', 'university', 'virginia', 'health', 'system', 'cancer', 'biophyics', 'hubrecht', 'institute', 'university', 'medical', 'center', 'utrecht', 'mathematics', 'case', 'western', 'reserve', 'university', 'developmental', 'biology', 'memorial', 'sloankettering', 'cancer', 'center', 'mathematics', 'statistics', 'georgia', 'state', 'university', 'heart', 'lung', 'research', 'institute', 'davis', 'heart', 'lung', 'research', 'institute', 'department', 'systems', 'biology', 'harvard', 'medical', 'school', 'department', 'anatomy', 'structural', 'biology', 'albert', 'einstein', 'college', 'medicine', 'nuclear', 'dynamics', 'babraham', 'institute', 'imaging', 'stowers', 'institute', 'medical', 'research', 'mathematics', 'university', 'british', 'columbia', 'mathematics', 'university', 'college', 'london', 'molecular', 'genetics', 'ohio', 'state', 'university'], tags='affiliation'), TaggedDocument(words=['password', 'used', 'authenticate', 'proxy', 'server', 'wakeup', 'time', 'seconds', 'pbid', 'run', 'checks', 'proxy', 'server', 'port', 'number', 'number', 'hours', 'representing', 'often', 'pbid', 'refreshes', 'index', 'meta', 'files', 'repos', 'default', 'every', '24', 'hours', 'proxy', 'server', 'ip', 'address', 'username', 'used', 'authenticate', 'proxy', 'server', 'http', 'socks5'], tags='description'), TaggedDocument(words=['840', '640', '40', '520', '00', '300', '350'], tags='position')]\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "cols = tagcol_paragraph_embeddings_features_nb(samples, labels[:10])\n",
    "\n",
    "print(cols[0])\n",
    "print(cols[0].words)\n",
    "print(cols[0].tags, '\\n')\n",
    "\n",
    "print(f'Tagged Columns Doc2Vec Model, process took {datetime.now() - start} seconds.', '\\n')\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec model in 400 dimensions\n",
      "Trained Doc2Vec Model, 400 dim, process took 0:00:00.101217 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "vec_dim = 400\n",
    "print(f'Training Doc2Vec model in {vec_dim} dimensions')\n",
    "\n",
    "train_paragraph_embeddings_features(cols, vec_dim)\n",
    "\n",
    "print(f'Trained Doc2Vec Model, {vec_dim} dim, process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2022-03-29 16:59:23.854041\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to save pkl + 3 npy files but missing one npy file\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Train Doc2Vec model\n",
    "\n",
    "train_model = Doc2Vec(\n",
    "    cols,\n",
    "    dm=0,\n",
    "    negative=3,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    vector_size=vec_dim,\n",
    "    epochs=2,\n",
    "    min_count=2,\n",
    "    seed=13,\n",
    ")\n",
    "\n",
    "# Save trained model\n",
    "#model_file = f\"../sherlock/features/par_vec_trained_{vec_dim}.pkl\"\n",
    "model_file = f\"../features/par_vec_trained_{vec_dim}.pkl\"\n",
    "\n",
    "train_model.save(model_file)\n",
    "train_model.delete_temporary_training_data(\n",
    "    keep_doctags_vectors=True, keep_inference=True\n",
    ")\n",
    "\n",
    "#print(train_model.docvecs.most_similar(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1604917e-03  2.5405362e-04 -4.7059624e-05 -5.8689527e-04\n",
      " -9.1203261e-04 -4.7114867e-04 -1.1892943e-04 -3.7765215e-04\n",
      " -6.9877534e-04 -9.8400656e-04  5.3689361e-04  1.1438956e-03\n",
      "  6.7346054e-04 -7.3486770e-04  9.7964285e-04 -4.0206686e-04\n",
      "  4.6434408e-04  8.1705773e-04  1.0198189e-03 -1.0774693e-04\n",
      "  8.5484330e-04 -1.1690875e-03 -1.6510856e-04  2.6501023e-04\n",
      "  7.9718063e-04 -7.8714511e-04  8.5567194e-04 -4.1868506e-04\n",
      " -3.7316902e-04 -7.7364163e-04 -1.1254011e-03 -9.4433711e-04\n",
      "  6.2381872e-04 -2.0078970e-04  9.5900912e-05 -9.7853318e-04\n",
      "  6.1046885e-04  8.6656638e-04 -6.1898585e-04  4.3883212e-04\n",
      " -3.7723588e-04 -2.1369127e-04 -1.1882943e-04 -1.6242257e-04\n",
      " -1.6104417e-04 -4.9740116e-05  4.3501714e-04  6.4576609e-04\n",
      " -9.9690782e-04 -1.8289508e-04  5.8654702e-04 -1.0299077e-03\n",
      "  8.9984504e-04 -1.8582179e-04  3.7929762e-04  7.6381104e-05\n",
      " -5.5838079e-04 -7.5596955e-04  1.2036795e-03  1.0680333e-03\n",
      "  8.8456413e-04  1.2139594e-03 -1.2422567e-03 -5.3944753e-04\n",
      " -8.0899731e-04 -1.0030410e-03 -5.1354035e-04 -1.0288442e-03\n",
      " -1.9502913e-04  5.9615989e-04 -4.8880139e-04  1.5011743e-04\n",
      " -4.4491322e-04  7.7642384e-04  7.6952518e-04 -9.7666627e-05\n",
      "  1.4112184e-04 -7.5857752e-06 -1.0129438e-03 -5.6932465e-04\n",
      " -4.3570568e-04  5.9480569e-04  1.1550724e-03  1.3182491e-05\n",
      " -1.1624927e-03 -2.1401564e-04 -6.4067182e-04  2.1785723e-04\n",
      " -3.9944480e-05 -2.1489969e-04 -1.4089911e-04  7.2227977e-04\n",
      "  3.1192115e-04 -8.3243987e-04  8.3126826e-04 -5.3110038e-04\n",
      "  6.6491798e-04  8.6339551e-04  8.7027013e-04 -1.0536808e-03\n",
      "  5.7417556e-04  9.2591287e-04  3.6650561e-04  1.0643112e-03\n",
      " -3.6421814e-04  9.4867731e-04 -1.4867574e-04 -5.2554830e-04\n",
      "  9.1393798e-04  5.1188550e-04 -3.7964547e-04 -8.8431267e-04\n",
      " -6.1858632e-04 -6.0943363e-04 -4.2760259e-04 -1.0299273e-03\n",
      " -7.3645491e-04 -8.3897082e-04 -6.0513109e-04  1.1036118e-04\n",
      "  5.0116790e-04  3.1981737e-04  7.7649180e-05  2.9215039e-04\n",
      "  1.0855834e-03 -1.0036772e-04  3.7391580e-04 -9.3903509e-04\n",
      " -1.4245928e-04  4.2353765e-04 -1.1094210e-03  5.6047912e-04\n",
      "  7.8161753e-04 -4.4985343e-05 -7.9898379e-04 -1.2421179e-04\n",
      "  1.0877926e-03  4.8677530e-04  1.1002255e-03  4.3945349e-04\n",
      "  4.2017975e-05 -8.0841919e-04 -1.2158654e-03 -1.4586260e-05\n",
      "  2.8713665e-04 -1.0271026e-04  4.3238883e-04  7.8166299e-04\n",
      " -4.6339745e-04 -2.5154118e-04  9.5620827e-04 -3.6091777e-04\n",
      "  1.1522842e-03  8.1504817e-04  9.7940688e-04  8.9524867e-04\n",
      " -8.5371942e-04  2.3292178e-05 -4.9660888e-05  6.6962454e-04\n",
      " -1.2075704e-03 -6.6501368e-04 -5.9950358e-04 -7.8567502e-04\n",
      " -1.1723577e-03 -1.7979527e-04 -8.8014343e-04 -7.8077510e-04\n",
      " -1.5073286e-04 -5.5000756e-04  1.1433883e-03  6.5764284e-04\n",
      "  8.0673053e-04  9.0959459e-04  4.2232338e-04  5.0257758e-04\n",
      " -1.0432292e-03  8.9415320e-04  7.8518299e-04  1.2024472e-03\n",
      "  1.5696275e-04 -4.8132130e-04  5.3008134e-04 -5.4791308e-04\n",
      "  5.9442763e-04  1.1914553e-03  8.6777192e-04 -1.1059268e-03\n",
      " -1.2262915e-03 -5.8975234e-04  1.0534583e-04 -4.7828656e-04\n",
      " -7.3804389e-05 -8.2886399e-04 -8.1651710e-04 -7.1154049e-05\n",
      "  9.0766215e-04  8.4779918e-04  1.1368878e-03 -4.0947364e-04\n",
      " -2.0783931e-05  6.7076425e-04  3.5068733e-05 -9.4483042e-04\n",
      "  2.2724092e-04 -5.9833948e-04 -1.1013004e-03 -6.8841095e-04\n",
      " -1.0223530e-03 -8.1163866e-04 -7.9600897e-04 -1.2341847e-03\n",
      "  1.1126089e-03 -8.4470847e-04 -9.8754384e-04 -3.2787048e-05\n",
      " -1.2407458e-03  7.5866131e-04 -3.9970822e-04  2.9265005e-04\n",
      " -1.4244969e-05 -9.6788222e-04  1.6512437e-04  9.0808270e-04\n",
      " -3.0710825e-04 -9.1448653e-04  1.8963045e-04 -2.8350367e-04\n",
      " -7.8690966e-04 -3.5581802e-04  6.8830693e-04 -1.0004797e-03\n",
      "  8.3736540e-04  4.0404255e-05  7.4720813e-04  6.0255505e-04\n",
      "  9.5436123e-04 -1.6382596e-04 -5.6686561e-04  1.0835828e-03\n",
      " -8.7775331e-04 -6.1447971e-04  1.2374095e-03  1.1060954e-03\n",
      " -8.3662861e-04 -1.1860065e-03  1.2455422e-03  9.5941912e-04\n",
      "  3.3180826e-04  8.8410178e-04 -1.0515600e-04  1.5592812e-04\n",
      " -1.2402043e-03 -5.2591902e-04 -8.9046385e-05 -8.1939908e-04\n",
      " -7.3245517e-04 -8.6626096e-04  1.5867972e-04 -5.9639715e-04\n",
      "  5.5520362e-05  1.1994359e-03  7.0218713e-04  5.2538497e-04\n",
      "  4.0710933e-04  3.1520198e-05  3.4553165e-04 -2.0449321e-05\n",
      "  1.0177032e-03  6.0872484e-05  1.2349881e-03 -8.5631356e-04\n",
      "  6.2615692e-04  4.4788769e-04  7.4451312e-04 -1.1073396e-03\n",
      " -1.1542520e-03 -8.3770306e-04 -1.1402832e-03  4.2397942e-04\n",
      "  4.1050656e-04  9.3550776e-04 -4.0363302e-04 -8.5344358e-04\n",
      " -1.2330940e-03  9.8944758e-04  1.0450835e-03 -5.7251367e-04\n",
      " -1.1410888e-03 -5.9099629e-04  7.7153259e-04 -5.3491577e-04\n",
      "  9.2257792e-04 -4.8256922e-04  4.6347760e-04  3.2393108e-04\n",
      " -7.0608378e-04  5.2450479e-05  7.9402816e-04 -6.1853777e-04\n",
      "  4.3844414e-04  2.0616164e-04  1.9177806e-04 -7.0908223e-04\n",
      "  8.5580064e-04  8.3390412e-05  4.4738833e-04 -3.6923957e-04\n",
      "  1.1056161e-03  5.1673281e-04  8.3517755e-04  6.1270769e-04\n",
      " -6.4682268e-04  6.4586243e-04 -3.2641031e-04 -9.3535666e-04\n",
      " -9.2727179e-04 -6.2929047e-04  4.1157802e-04  3.2464965e-04\n",
      "  3.0532037e-04 -1.4772352e-04 -9.6762822e-05  1.1017388e-03\n",
      " -3.4010477e-04 -1.0161987e-03  5.4845377e-04  5.1194587e-04\n",
      "  8.0796750e-04  5.9164461e-04  1.0096897e-04  5.8499957e-04\n",
      "  4.8173446e-04 -1.2073129e-03 -5.4589345e-05 -3.1867056e-04\n",
      "  1.0603972e-03  6.0030399e-04  5.4469879e-04  1.0032057e-03\n",
      " -7.1973284e-04  1.1383052e-03  5.8193482e-04  1.1484334e-03\n",
      " -8.0682890e-04  1.2497004e-03  6.7884312e-04 -5.3620921e-04\n",
      "  1.0723242e-03 -2.1976931e-04 -5.0210109e-04 -9.0481329e-04\n",
      " -4.0211515e-05  8.1823580e-04  3.6460569e-05 -8.2662515e-04\n",
      " -1.1475668e-03  1.0505802e-03 -5.0970906e-04 -1.2162462e-03\n",
      "  2.0034304e-04  1.0398335e-03  1.1996797e-03  5.8841551e-06\n",
      "  3.0982622e-04 -1.0899079e-03 -5.8543810e-04  8.4424205e-04\n",
      "  5.8676692e-04 -8.2156284e-06  1.2691972e-04 -5.6690554e-04\n",
      " -1.0630031e-03  8.0355181e-04 -1.0675369e-03 -1.5236717e-04\n",
      " -7.9358421e-04  9.7118737e-04  1.7357973e-04  3.6953570e-04\n",
      "  3.9983043e-04  5.9589907e-04 -1.1400845e-03  4.2368169e-04\n",
      " -8.5198204e-04  2.7433931e-04  1.4809571e-04 -4.1806514e-05\n",
      " -2.9049270e-04  3.5867831e-04  9.3456503e-04 -1.1002580e-03\n",
      "  3.3961763e-04 -3.8627448e-04 -5.5194600e-04  5.8950001e-04\n",
      "  1.5892429e-04 -6.9004868e-04  7.1986935e-05  9.0638932e-04]\n"
     ]
    }
   ],
   "source": [
    "doc_words1 = [\"last\", \"Deployment\" ,\"early\" ,\"other\" ,\"the work\", \"impact\", \"receive\" ,\"Behind the back\" ,\"Tsukuri\", \"trick\" ,\"Every time\" ,\"thing\", \"Take off your hat\", \"To do\", \"Read\", \"Cheap\" ,\"Me\" ,\"Mystery\"]\n",
    "doc_words2 = [ \"Initiation love\", \"Similarly\" ,\"last\", \"A few lines\", \"Plot twist\", \"Go\", \"Time\", \"Time\", \"various\", \"scene\", \"To do\" ,\"To be\", \"Foreshadowing\" ,\"Sprinkle\", \"らTo be\" ,\"Is\", \"thing\", \"notice\"]\n",
    "\n",
    "doc_words3 = [\"computer\", \"it\", \"science\",\"python\", \"data\", \"database\"]\n",
    "doc_words4 = [\"python\", \"data\", \"database\",\"computer\", \"it\", \"science\"]\n",
    "\n",
    "# convert test/unseen paragraph to vector\n",
    "print(train_model.infer_vector(doc_words1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_model.docvecs.most_similar(0))\n",
    "\n",
    "\n",
    "\n",
    "sim_value =train_model.docvecs.similarity_unseen_docs(train_model, doc_words1, doc_words4, alpha=1, min_alpha=0.0001, steps=5)\n",
    "print(sim_value)\n",
    "\n",
    "sim_value =train_model.docvecs.similarity_unseen_docs(train_model, doc_words3, doc_words4, alpha=1, min_alpha=0.0001, steps=5)\n",
    "print(sim_value)\n",
    "\n",
    "print(cols[0])\n",
    "\n",
    "\n",
    "print(train_model)\n",
    "print(train_model.docvecs.most_similar(0))\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(model.docvecs.most_similar('0'))\n",
    "\n",
    "\n",
    "print(train_model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "print(train_model.most_similar(positive=['country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunny/miniconda3/envs/env5/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunny/miniconda3/envs/env5/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n",
      "V1_infer [ 0.0165429  -0.00657688 -0.02399274 -0.01181185  0.00387255 -0.02063944\n",
      " -0.01466336 -0.00024349  0.01962049 -0.02046933 -0.03034732  0.0260456\n",
      " -0.0088838  -0.01737959 -0.01878665 -0.00285814  0.02368273 -0.01362796\n",
      " -0.00856395 -0.0003635 ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2559/2354564209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# to find most similar doc using tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msimilar_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'animal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env5/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, indexer)\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0mall_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "    \n",
    "#model_file = f\"../sherlock/features/par_vec_trained_{vec_dim}.pkl\"\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 0.0165429  -0.00657688 -0.02399274 -0.01181185  0.00387255 -0.02063944\n",
      " -0.01466336 -0.00024349  0.01962049 -0.02046933 -0.03034732  0.0260456\n",
      " -0.0088838  -0.01737959 -0.01878665 -0.00285814  0.02368273 -0.01362796\n",
      " -0.00856395 -0.0003635 ]\n",
      "[('0', 0.9920483231544495), ('2', 0.9915789365768433), ('3', 0.9913952350616455)]\n",
      "[-0.20910682  0.27863237 -0.21247207  0.00621525  0.44725454  0.10419783\n",
      " -0.12027526 -0.07403916 -0.15418532  0.1021312  -0.2671689   0.47951084\n",
      "  0.2586121   0.12724744  0.25313672  0.09417614  0.6556045  -0.13270836\n",
      "  0.09749547  0.1739856 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "067fbafb8497a6b213015d42733fcc9edf67b269b0a1726470d212d235a61220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
