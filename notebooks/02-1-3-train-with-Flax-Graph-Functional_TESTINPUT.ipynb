{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.deploy import helpers\n",
    "import graphviz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cc932",
   "metadata": {},
   "source": [
    "## Download & Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af3a822",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msherlock\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparagraph_vectors\u001b[39;00m \u001b[39mimport\u001b[39;00m initialise_pretrained_model, initialise_nltk\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msherlock\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=2'>3</a>\u001b[0m     extract_features,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=3'>4</a>\u001b[0m     convert_string_lists_to_lists,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=4'>5</a>\u001b[0m     prepare_feature_extraction,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m     load_parquet_values,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/GitCode/sherlock-project/notebooks/02-1-3-train-with-Flax-Graph-Functional.ipynb#ch0000002vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msherlock\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mword_embeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m initialise_word_embeddings\n",
      "File \u001b[0;32m/mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      <a href='file:///mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\n\u001b[0;32m----> <a href='file:///mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdoc2vec\u001b[39;00m\n\u001b[1;32m      <a href='file:///mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      <a href='file:///mnt/d/GitCode/sherlock-project/sherlock/features/paragraph_vectors.py?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "# from sherlock.features.preprocessing import (\n",
    "#     extract_features,\n",
    "#     convert_string_lists_to_lists,\n",
    "#     prepare_feature_extraction,\n",
    "#     load_parquet_values,\n",
    "# )\n",
    "\n",
    "# from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "\n",
    "\n",
    "# prepare_feature_extraction()\n",
    "# initialise_word_embeddings()\n",
    "# initialise_pretrained_model(400)\n",
    "# initialise_nltk()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data process took 0:00:33.609518 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "\n",
    "X_train = pd.read_parquet('../data/data/processed_golden/train.parquet')\n",
    "y_train = pd.read_parquet('../data/data/raw/train_labels.parquet').values.flatten()\n",
    "y_train = np.array([x.lower() for x in y_train])\n",
    "\n",
    "X_validation = pd.read_parquet('../data/data/processed/validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/data/raw/val_labels.parquet').values.flatten()\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/test.parquet')\n",
    "y_test = pd.read_parquet('../data/data/raw/test_labels.parquet').values.flatten()\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff95873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(y_train))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_train_char = X_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_validation[feature_cols[\"char\"]]\n",
    "X_val_word = X_validation[feature_cols[\"word\"]]\n",
    "X_val_par = X_validation[feature_cols[\"par\"]]\n",
    "X_val_rest = X_validation[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_train)   #(412059,)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(412059,78)\n",
    "\n",
    "y_val_int = encoder.transform(y_validation)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccd356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588\n"
     ]
    }
   ],
   "source": [
    "#X_train.shape\n",
    "\n",
    "print(960+201+400+27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afa8a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "j_1 = jnp.array(pd.DataFrame(X_train_char).to_numpy())\n",
    "j_2 = jnp.array(pd.DataFrame(X_train_word).to_numpy())\n",
    "j_3 = jnp.array(pd.DataFrame(X_train_par).to_numpy())\n",
    "j_4 = jnp.array(pd.DataFrame(X_train_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33df66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "  \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f00e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    training: bool = True\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # batchnormalisation - https://github.com/google/flax/issues/932\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        x = nn.relu(nn.Dense(self.features[0])(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "                \n",
    "        x = nn.relu(nn.Dense(self.features[1])(x)) \n",
    "        \n",
    "        # todo: add \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b45c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[4146024105  967050713]\n",
      "[0.14389051]\n",
      "[-2.6105583   0.03385283  1.0863333  -1.4802988   0.48895672  1.062516\n",
      "  0.54174834  0.0170228   0.2722685   0.30522448]\n"
     ]
    }
   ],
   "source": [
    "# PRNGKey Example\n",
    "print(random.PRNGKey(0))\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "print(key1)\n",
    "print(random.normal(key1,shape=(1,)))\n",
    "\n",
    "a = random.normal(key1, (10,))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee3435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    num_classes: int = 78\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x1, x2, x3, x4):\n",
    "       \n",
    "        # [1] define shape        \n",
    "        y1 = SubModel([300, 300], name='char_model')(x1)      \n",
    "        y2 = SubModel([200, 200], name='word_model')(x2)\n",
    "        y3 = SubModel([400, 400], name='par_model')(x3)\n",
    "        y4 = RestModel([27], name='rest_model')(x4)\n",
    "                      \n",
    "        # [2] concat submodels    \n",
    "        x = jnp.concatenate((y1, y2, y3, y4), axis=-1)\n",
    "        \n",
    "        print(\"check mainmodel shape\")\n",
    "        print(np.shape(x))\n",
    "        \n",
    "        # batchnormalisation\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        # dense 1\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "        \n",
    "        # dense 2\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dense w/ softmax - todo: check\n",
    "        x = nn.softmax(nn.Dense(self.feature_size)(x), axis=-1)\n",
    "        \n",
    "        return nn.Dense(self.num_classes)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "590cbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "mainmodel = MainModel()\n",
    "p_main = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27))) \n",
    "#(1, 927)\n",
    "\n",
    "\n",
    "\n",
    "# ??\n",
    "# it's not actual training part ( training = optax)\n",
    "# should I use jnp.ones((1, 960)) instead of actual data for faster computation\n",
    "\n",
    "\n",
    "#y_main = mainmodel.apply(p_main,j_1,j_2,j_3,j_4)\n",
    "#(412059, 927)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ffb6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['char_model', 'word_model', 'par_model', 'rest_model', 'BatchNorm_0', 'Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p_main\n",
    "p_main['params'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9325b0",
   "metadata": {},
   "source": [
    "## Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42898abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn_outcome.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowered = jax.jit(mainmodel.apply).lower(p_main,j_1,j_2,j_3,j_4)\n",
    "comp_dot = graphviz.Source(lowered._xla_computation().as_hlo_dot_graph())\n",
    "comp_dot.render('nn_outcome', view=True).replace('\\\\', '/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b0bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412059, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"application/pdf\"\n"
     ]
    }
   ],
   "source": [
    "print(y_main.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf381d",
   "metadata": {},
   "source": [
    "## Training - OPTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func(x_b_1, x_b_2, x_b_3, x_b_4, y_batched):\n",
    "  def mse(p_main):    \n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x1, x2, x3, x4, y):      \n",
    "      pred = mainmodel.apply(p_main, x1, x2, x3, x4)\n",
    "      return jnp.inner(y-pred, y-pred) / 2.0  \n",
    "    \n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_b_1, x_b_2, x_b_3, x_b_4, y_batched), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d7f41a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"application/pdf\"\n"
     ]
    }
   ],
   "source": [
    "params = p_main\n",
    "\n",
    "#dst_x = jnp.concatenate((j_1, j_2, j_3, j_4), axis=-1)\n",
    "dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "loss = make_mse_func(j_1, j_2, j_3, j_4, dst_y)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22217fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] = 0.49831196665763855\n",
      "Loss[1] = 0.49801039695739746\n",
      "Loss[2] = 0.4976503551006317\n",
      "Loss[3] = 0.4971697926521301\n",
      "Loss[4] = 0.4968632459640503\n",
      "Loss[5] = 0.4965384304523468\n",
      "Loss[6] = 0.4963408410549164\n",
      "Loss[7] = 0.49597835540771484\n",
      "Loss[8] = 0.4956151843070984\n",
      "Loss[9] = 0.49521604180336\n",
      "Loss[10] = 0.4947943091392517\n",
      "Loss[11] = 0.4948485195636749\n",
      "Loss[12] = 0.4947710633277893\n",
      "Loss[13] = 0.49449214339256287\n",
      "Loss[14] = 0.49395594000816345\n",
      "Loss[15] = 0.49364984035491943\n",
      "Loss[16] = 0.493425190448761\n",
      "Loss[17] = 0.4933165907859802\n",
      "Loss[18] = 0.4930729866027832\n",
      "Loss[19] = 0.492763489484787\n",
      "Loss[20] = 0.4924956262111664\n",
      "Loss[21] = 0.49233394861221313\n",
      "Loss[22] = 0.4922170042991638\n",
      "Loss[23] = 0.49192357063293457\n",
      "Loss[24] = 0.4917111396789551\n",
      "Loss[25] = 0.4913980960845947\n",
      "Loss[26] = 0.49119460582733154\n",
      "Loss[27] = 0.4909331500530243\n",
      "Loss[28] = 0.4907146990299225\n",
      "Loss[29] = 0.49051517248153687\n",
      "Loss[30] = 0.4902375042438507\n",
      "Loss[31] = 0.48994067311286926\n",
      "Loss[32] = 0.48973897099494934\n",
      "Loss[33] = 0.4895220696926117\n",
      "Loss[34] = 0.4892893135547638\n",
      "Loss[35] = 0.48903989791870117\n",
      "Loss[36] = 0.488864928483963\n",
      "Loss[37] = 0.4885769188404083\n",
      "Loss[38] = 0.4883756935596466\n",
      "Loss[39] = 0.48818162083625793\n",
      "Loss[40] = 0.48797720670700073\n",
      "Loss[41] = 0.48775210976600647\n",
      "Loss[42] = 0.48749396204948425\n",
      "Loss[43] = 0.48724767565727234\n",
      "Loss[44] = 0.48699626326560974\n",
      "Loss[45] = 0.486753910779953\n",
      "Loss[46] = 0.48645955324172974\n",
      "Loss[47] = 0.48619547486305237\n",
      "Loss[48] = 0.4864181578159332\n",
      "Loss[49] = 0.4861951172351837\n",
      "Loss[50] = 0.4856349527835846\n",
      "Loss[51] = 0.48545771837234497\n",
      "Loss[52] = 0.48524340987205505\n",
      "Loss[53] = 0.48500052094459534\n",
      "Loss[54] = 0.4847262501716614\n",
      "Loss[55] = 0.48445332050323486\n",
      "Loss[56] = 0.48417627811431885\n",
      "Loss[57] = 0.4838867783546448\n",
      "Loss[58] = 0.4835825264453888\n",
      "Loss[59] = 0.483350545167923\n",
      "Loss[60] = 0.4830663800239563\n",
      "Loss[61] = 0.4827812910079956\n",
      "Loss[62] = 0.4824884831905365\n",
      "Loss[63] = 0.48219984769821167\n",
      "Loss[64] = 0.48189595341682434\n",
      "Loss[65] = 0.4815904200077057\n",
      "Loss[66] = 0.48128142952919006\n",
      "Loss[67] = 0.4809610843658447\n",
      "Loss[68] = 0.48062095046043396\n",
      "Loss[69] = 0.4805523753166199\n",
      "Loss[70] = 0.4801226258277893\n",
      "Loss[71] = 0.47995421290397644\n",
      "Loss[72] = 0.4797203540802002\n",
      "Loss[73] = 0.47945284843444824\n",
      "Loss[74] = 0.47920042276382446\n",
      "Loss[75] = 0.4788548946380615\n",
      "Loss[76] = 0.4785556197166443\n",
      "Loss[77] = 0.47821271419525146\n",
      "Loss[78] = 0.4778234660625458\n",
      "Loss[79] = 0.4774874746799469\n",
      "Loss[80] = 0.47710397839546204\n",
      "Loss[81] = 0.4767376482486725\n",
      "Loss[82] = 0.4763975143432617\n",
      "Loss[83] = 0.4760356843471527\n",
      "Loss[84] = 0.47566521167755127\n",
      "Loss[85] = 0.4753265976905823\n",
      "Loss[86] = 0.47493404150009155\n",
      "Loss[87] = 0.4745786190032959\n",
      "Loss[88] = 0.4741845726966858\n",
      "Loss[89] = 0.4738192856311798\n",
      "Loss[90] = 0.4734877943992615\n",
      "Loss[91] = 0.4731103479862213\n",
      "Loss[92] = 0.47270819544792175\n",
      "Loss[93] = 0.4723691940307617\n",
      "Loss[94] = 0.47200241684913635\n",
      "Loss[95] = 0.47159865498542786\n",
      "Loss[96] = 0.471225768327713\n",
      "Loss[97] = 0.4708485007286072\n",
      "Loss[98] = 0.470515638589859\n",
      "Loss[99] = 0.4701026678085327\n",
      "Loss[100] = 0.4697583019733429\n",
      "Loss[101] = 0.4693763256072998\n",
      "Loss[102] = 0.46897006034851074\n",
      "Loss[103] = 0.46862921118736267\n",
      "Loss[104] = 0.46822983026504517\n",
      "Loss[105] = 0.46787577867507935\n",
      "Loss[106] = 0.467501699924469\n",
      "Loss[107] = 0.4671528935432434\n",
      "Loss[108] = 0.4667874574661255\n",
      "Loss[109] = 0.46646127104759216\n",
      "Loss[110] = 0.4660840630531311\n",
      "Loss[111] = 0.4657295048236847\n",
      "Loss[112] = 0.4653901755809784\n",
      "Loss[113] = 0.46506521105766296\n",
      "Loss[114] = 0.4647414982318878\n",
      "Loss[115] = 0.4643646478652954\n",
      "Loss[116] = 0.46413949131965637\n",
      "Loss[117] = 0.4636901319026947\n",
      "Loss[118] = 0.4633479416370392\n",
      "Loss[119] = 0.4630167484283447\n",
      "Loss[120] = 0.46265313029289246\n",
      "Loss[121] = 0.4623400866985321\n",
      "Loss[122] = 0.46198004484176636\n",
      "Loss[123] = 0.46166208386421204\n",
      "Loss[124] = 0.46133607625961304\n",
      "Loss[125] = 0.4610064923763275\n",
      "Loss[126] = 0.46067366003990173\n",
      "Loss[127] = 0.4603453576564789\n",
      "Loss[128] = 0.4600168764591217\n",
      "Loss[129] = 0.45969560742378235\n",
      "Loss[130] = 0.459355890750885\n",
      "Loss[131] = 0.45905381441116333\n",
      "Loss[132] = 0.4587540328502655\n",
      "Loss[133] = 0.45845362544059753\n",
      "Loss[134] = 0.4581531584262848\n",
      "Loss[135] = 0.4578821063041687\n",
      "Loss[136] = 0.4575801491737366\n",
      "Loss[137] = 0.4573047459125519\n",
      "Loss[138] = 0.45702674984931946\n",
      "Loss[139] = 0.45667415857315063\n",
      "Loss[140] = 0.4564022719860077\n",
      "Loss[141] = 0.45608776807785034\n",
      "Loss[142] = 0.45574459433555603\n",
      "Loss[143] = 0.4556368589401245\n",
      "Loss[144] = 0.45521676540374756\n",
      "Loss[145] = 0.454959899187088\n",
      "Loss[146] = 0.4546906650066376\n",
      "Loss[147] = 0.45440080761909485\n",
      "Loss[148] = 0.454102486371994\n",
      "Loss[149] = 0.45381444692611694\n",
      "Loss[150] = 0.45352330803871155\n",
      "Loss[151] = 0.4532364308834076\n",
      "Loss[152] = 0.4529220163822174\n",
      "Loss[153] = 0.45265212655067444\n",
      "Loss[154] = 0.452373743057251\n",
      "Loss[155] = 0.4520763158798218\n",
      "Loss[156] = 0.45181378722190857\n",
      "Loss[157] = 0.4514787495136261\n",
      "Loss[158] = 0.45120012760162354\n",
      "Loss[159] = 0.450913667678833\n",
      "Loss[160] = 0.4506360590457916\n",
      "Loss[161] = 0.4503275752067566\n",
      "Loss[162] = 0.45009443163871765\n",
      "Loss[163] = 0.44981199502944946\n",
      "Loss[164] = 0.4495333433151245\n",
      "Loss[165] = 0.44922277331352234\n",
      "Loss[166] = 0.44895076751708984\n",
      "Loss[167] = 0.44871988892555237\n",
      "Loss[168] = 0.44843918085098267\n",
      "Loss[169] = 0.4481534957885742\n",
      "Loss[170] = 0.4478472173213959\n",
      "Loss[171] = 0.44759997725486755\n",
      "Loss[172] = 0.44733500480651855\n",
      "Loss[173] = 0.44705840945243835\n",
      "Loss[174] = 0.4467903673648834\n",
      "Loss[175] = 0.44648849964141846\n",
      "Loss[176] = 0.44619935750961304\n",
      "Loss[177] = 0.445946604013443\n",
      "Loss[178] = 0.44563835859298706\n",
      "Loss[179] = 0.44541245698928833\n",
      "Loss[180] = 0.4451504647731781\n",
      "Loss[181] = 0.444865882396698\n",
      "Loss[182] = 0.44462850689888\n",
      "Loss[183] = 0.4443517327308655\n",
      "Loss[184] = 0.44406643509864807\n",
      "Loss[185] = 0.4437931776046753\n",
      "Loss[186] = 0.4435308575630188\n",
      "Loss[187] = 0.443271666765213\n",
      "Loss[188] = 0.443004846572876\n",
      "Loss[189] = 0.44274184107780457\n",
      "Loss[190] = 0.4424615204334259\n",
      "Loss[191] = 0.4422270357608795\n",
      "Loss[192] = 0.4419497549533844\n",
      "Loss[193] = 0.4416916072368622\n",
      "Loss[194] = 0.44144079089164734\n",
      "Loss[195] = 0.4411926865577698\n",
      "Loss[196] = 0.4409281611442566\n",
      "Loss[197] = 0.4406750500202179\n",
      "Loss[198] = 0.4404364228248596\n",
      "Loss[199] = 0.44015464186668396\n",
      "Loss[200] = 0.43991267681121826\n",
      "Loss[201] = 0.43966707587242126\n",
      "Loss[202] = 0.439399778842926\n",
      "Loss[203] = 0.4391282796859741\n",
      "Loss[204] = 0.4388677179813385\n",
      "Loss[205] = 0.43860650062561035\n",
      "Loss[206] = 0.43835121393203735\n",
      "Loss[207] = 0.4380929470062256\n",
      "Loss[208] = 0.43784788250923157\n",
      "Loss[209] = 0.4376603960990906\n",
      "Loss[210] = 0.43742474913597107\n",
      "Loss[211] = 0.4371989071369171\n",
      "Loss[212] = 0.436948299407959\n",
      "Loss[213] = 0.4367188811302185\n",
      "Loss[214] = 0.4364694356918335\n",
      "Loss[215] = 0.43622922897338867\n",
      "Loss[216] = 0.4359651207923889\n",
      "Loss[217] = 0.4357268214225769\n",
      "Loss[218] = 0.4355034828186035\n",
      "Loss[219] = 0.4352338910102844\n",
      "Loss[220] = 0.43498238921165466\n",
      "Loss[221] = 0.4347574710845947\n",
      "Loss[222] = 0.43453487753868103\n",
      "Loss[223] = 0.43430501222610474\n",
      "Loss[224] = 0.4340943992137909\n",
      "Loss[225] = 0.43383654952049255\n",
      "Loss[226] = 0.43362316489219666\n",
      "Loss[227] = 0.43335580825805664\n",
      "Loss[228] = 0.43313124775886536\n",
      "Loss[229] = 0.4328659772872925\n",
      "Loss[230] = 0.43261614441871643\n",
      "Loss[231] = 0.43238240480422974\n",
      "Loss[232] = 0.4321482181549072\n",
      "Loss[233] = 0.43191736936569214\n",
      "Loss[234] = 0.43165338039398193\n",
      "Loss[235] = 0.43141812086105347\n",
      "Loss[236] = 0.43118909001350403\n",
      "Loss[237] = 0.43095317482948303\n",
      "Loss[238] = 0.43072596192359924\n",
      "Loss[239] = 0.4304824769496918\n",
      "Loss[240] = 0.43026334047317505\n",
      "Loss[241] = 0.4300360381603241\n",
      "Loss[242] = 0.4298076331615448\n",
      "Loss[243] = 0.429572194814682\n",
      "Loss[244] = 0.42933419346809387\n",
      "Loss[245] = 0.4291175603866577\n",
      "Loss[246] = 0.4289267659187317\n",
      "Loss[247] = 0.4286959171295166\n",
      "Loss[248] = 0.42845648527145386\n",
      "Loss[249] = 0.4282216727733612\n",
      "Loss[250] = 0.42800217866897583\n",
      "Loss[251] = 0.4277864396572113\n",
      "Loss[252] = 0.42755213379859924\n",
      "Loss[253] = 0.4273037314414978\n",
      "Loss[254] = 0.42711588740348816\n",
      "Loss[255] = 0.4268636703491211\n",
      "Loss[256] = 0.426636666059494\n",
      "Loss[257] = 0.42638230323791504\n",
      "Loss[258] = 0.4261825978755951\n",
      "Loss[259] = 0.4259651005268097\n",
      "Loss[260] = 0.4257736802101135\n",
      "Loss[261] = 0.4255297780036926\n",
      "Loss[262] = 0.42534369230270386\n",
      "Loss[263] = 0.42512282729148865\n",
      "Loss[264] = 0.4248725473880768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[265] = 0.42467013001441956\n",
      "Loss[266] = 0.4244250953197479\n",
      "Loss[267] = 0.4241901636123657\n",
      "Loss[268] = 0.4240042269229889\n",
      "Loss[269] = 0.4237840473651886\n",
      "Loss[270] = 0.42357128858566284\n",
      "Loss[271] = 0.42337942123413086\n",
      "Loss[272] = 0.4231986701488495\n",
      "Loss[273] = 0.422951877117157\n",
      "Loss[274] = 0.42277979850769043\n",
      "Loss[275] = 0.4224996864795685\n",
      "Loss[276] = 0.4222998321056366\n",
      "Loss[277] = 0.42210355401039124\n",
      "Loss[278] = 0.42185601592063904\n",
      "Loss[279] = 0.42167192697525024\n",
      "Loss[280] = 0.4214472472667694\n",
      "Loss[281] = 0.4212101101875305\n",
      "Loss[282] = 0.42106521129608154\n",
      "Loss[283] = 0.4207966923713684\n",
      "Loss[284] = 0.42067161202430725\n",
      "Loss[285] = 0.42045238614082336\n",
      "Loss[286] = 0.4202604591846466\n",
      "Loss[287] = 0.4200775921344757\n",
      "Loss[288] = 0.41987237334251404\n",
      "Loss[289] = 0.4196465015411377\n",
      "Loss[290] = 0.4194532036781311\n",
      "Loss[291] = 0.41922032833099365\n",
      "Loss[292] = 0.41902029514312744\n",
      "Loss[293] = 0.41879406571388245\n",
      "Loss[294] = 0.41858622431755066\n",
      "Loss[295] = 0.4183846116065979\n",
      "Loss[296] = 0.41818541288375854\n",
      "Loss[297] = 0.4179595112800598\n",
      "Loss[298] = 0.4177655875682831\n",
      "Loss[299] = 0.4175247848033905\n",
      "Loss[300] = 0.4173426330089569\n",
      "Loss[301] = 0.41713085770606995\n",
      "Loss[302] = 0.4169137179851532\n",
      "Loss[303] = 0.41670724749565125\n",
      "Loss[304] = 0.41649606823921204\n",
      "Loss[305] = 0.4162679612636566\n",
      "Loss[306] = 0.41608527302742004\n",
      "Loss[307] = 0.415865033864975\n",
      "Loss[308] = 0.41564998030662537\n",
      "Loss[309] = 0.4154418706893921\n",
      "Loss[310] = 0.41523414850234985\n",
      "Loss[311] = 0.41506874561309814\n",
      "Loss[312] = 0.41485124826431274\n",
      "Loss[313] = 0.41461849212646484\n",
      "Loss[314] = 0.4144376218318939\n",
      "Loss[315] = 0.4142073392868042\n",
      "Loss[316] = 0.41400110721588135\n",
      "Loss[317] = 0.413786917924881\n",
      "Loss[318] = 0.4135917127132416\n",
      "Loss[319] = 0.4134199917316437\n",
      "Loss[320] = 0.4131840467453003\n",
      "Loss[321] = 0.4129716753959656\n",
      "Loss[322] = 0.41277095675468445\n",
      "Loss[323] = 0.41258731484413147\n",
      "Loss[324] = 0.4123666286468506\n",
      "Loss[325] = 0.41217634081840515\n",
      "Loss[326] = 0.4119674861431122\n",
      "Loss[327] = 0.41178518533706665\n",
      "Loss[328] = 0.4115801751613617\n",
      "Loss[329] = 0.4113626778125763\n",
      "Loss[330] = 0.41117778420448303\n",
      "Loss[331] = 0.4109877943992615\n",
      "Loss[332] = 0.4107675552368164\n",
      "Loss[333] = 0.41056954860687256\n",
      "Loss[334] = 0.4103742241859436\n",
      "Loss[335] = 0.4101545512676239\n",
      "Loss[336] = 0.4099681079387665\n",
      "Loss[337] = 0.40976449847221375\n",
      "Loss[338] = 0.4095323085784912\n",
      "Loss[339] = 0.409369558095932\n",
      "Loss[340] = 0.4091693162918091\n",
      "Loss[341] = 0.4089585244655609\n",
      "Loss[342] = 0.40875670313835144\n",
      "Loss[343] = 0.40855854749679565\n",
      "Loss[344] = 0.40833786129951477\n",
      "Loss[345] = 0.40816208720207214\n",
      "Loss[346] = 0.40793120861053467\n",
      "Loss[347] = 0.4077138602733612\n",
      "Loss[348] = 0.4075245261192322\n",
      "Loss[349] = 0.40729913115501404\n",
      "Loss[350] = 0.4070993661880493\n",
      "Loss[351] = 0.40689948201179504\n",
      "Loss[352] = 0.40668925642967224\n",
      "Loss[353] = 0.4064883589744568\n",
      "Loss[354] = 0.4062860608100891\n",
      "Loss[355] = 0.40606003999710083\n",
      "Loss[356] = 0.40588995814323425\n",
      "Loss[357] = 0.40568238496780396\n",
      "Loss[358] = 0.40547335147857666\n",
      "Loss[359] = 0.40527889132499695\n",
      "Loss[360] = 0.405097633600235\n",
      "Loss[361] = 0.4048784673213959\n",
      "Loss[362] = 0.40468332171440125\n",
      "Loss[363] = 0.4044797122478485\n",
      "Loss[364] = 0.4042893648147583\n",
      "Loss[365] = 0.4040880799293518\n",
      "Loss[366] = 0.4038819372653961\n",
      "Loss[367] = 0.40368860960006714\n",
      "Loss[368] = 0.4034879803657532\n",
      "Loss[369] = 0.4032849371433258\n",
      "Loss[370] = 0.40310436487197876\n",
      "Loss[371] = 0.40291690826416016\n",
      "Loss[372] = 0.40273746848106384\n",
      "Loss[373] = 0.4025404155254364\n",
      "Loss[374] = 0.40232089161872864\n",
      "Loss[375] = 0.4021051228046417\n",
      "Loss[376] = 0.40189099311828613\n",
      "Loss[377] = 0.40170061588287354\n",
      "Loss[378] = 0.4015161097049713\n",
      "Loss[379] = 0.4013136625289917\n",
      "Loss[380] = 0.4011217951774597\n",
      "Loss[381] = 0.4009113609790802\n",
      "Loss[382] = 0.40071895718574524\n",
      "Loss[383] = 0.40055322647094727\n",
      "Loss[384] = 0.40032294392585754\n",
      "Loss[385] = 0.40014514327049255\n",
      "Loss[386] = 0.39998558163642883\n",
      "Loss[387] = 0.3997827172279358\n",
      "Loss[388] = 0.3995710611343384\n",
      "Loss[389] = 0.39939311146736145\n",
      "Loss[390] = 0.3991778790950775\n",
      "Loss[391] = 0.39898937940597534\n",
      "Loss[392] = 0.3988082706928253\n",
      "Loss[393] = 0.3985547423362732\n",
      "Loss[394] = 0.3983984589576721\n",
      "Loss[395] = 0.39816808700561523\n",
      "Loss[396] = 0.3979768455028534\n",
      "Loss[397] = 0.3978097438812256\n",
      "Loss[398] = 0.397591769695282\n",
      "Loss[399] = 0.39738985896110535\n",
      "Loss[400] = 0.3971838653087616\n",
      "Loss[401] = 0.39700356125831604\n",
      "Loss[402] = 0.3967582583427429\n",
      "Loss[403] = 0.3965590298175812\n",
      "Loss[404] = 0.39637064933776855\n",
      "Loss[405] = 0.3962033689022064\n",
      "Loss[406] = 0.39597105979919434\n",
      "Loss[407] = 0.395772784948349\n",
      "Loss[408] = 0.39555591344833374\n",
      "Loss[409] = 0.39534705877304077\n",
      "Loss[410] = 0.39519503712654114\n",
      "Loss[411] = 0.3950050473213196\n",
      "Loss[412] = 0.39488428831100464\n",
      "Loss[413] = 0.3946866989135742\n",
      "Loss[414] = 0.39444246888160706\n",
      "Loss[415] = 0.3943178653717041\n",
      "Loss[416] = 0.3940669894218445\n",
      "Loss[417] = 0.39384233951568604\n",
      "Loss[418] = 0.39368370175361633\n",
      "Loss[419] = 0.39345884323120117\n",
      "Loss[420] = 0.39328187704086304\n",
      "Loss[421] = 0.39309996366500854\n",
      "Loss[422] = 0.3928922414779663\n",
      "Loss[423] = 0.3926931917667389\n",
      "Loss[424] = 0.3924699127674103\n",
      "Loss[425] = 0.39226073026657104\n",
      "Loss[426] = 0.39202234148979187\n",
      "Loss[427] = 0.39182761311531067\n",
      "Loss[428] = 0.39158639311790466\n",
      "Loss[429] = 0.39140307903289795\n",
      "Loss[430] = 0.3911927044391632\n",
      "Loss[431] = 0.3909691572189331\n",
      "Loss[432] = 0.39077702164649963\n",
      "Loss[433] = 0.39055800437927246\n",
      "Loss[434] = 0.39036181569099426\n",
      "Loss[435] = 0.39014825224876404\n",
      "Loss[436] = 0.38993385434150696\n",
      "Loss[437] = 0.389724463224411\n",
      "Loss[438] = 0.3895200490951538\n",
      "Loss[439] = 0.38931721448898315\n",
      "Loss[440] = 0.38911622762680054\n",
      "Loss[441] = 0.38890597224235535\n",
      "Loss[442] = 0.3887037932872772\n",
      "Loss[443] = 0.3885023891925812\n",
      "Loss[444] = 0.38830673694610596\n",
      "Loss[445] = 0.3880934715270996\n",
      "Loss[446] = 0.3879019021987915\n",
      "Loss[447] = 0.38769277930259705\n",
      "Loss[448] = 0.3874918818473816\n",
      "Loss[449] = 0.38729703426361084\n",
      "Loss[450] = 0.38709449768066406\n",
      "Loss[451] = 0.38691118359565735\n",
      "Loss[452] = 0.3867127597332001\n",
      "Loss[453] = 0.3865062892436981\n",
      "Loss[454] = 0.3863152861595154\n",
      "Loss[455] = 0.3861106038093567\n",
      "Loss[456] = 0.38592904806137085\n",
      "Loss[457] = 0.3857238292694092\n",
      "Loss[458] = 0.385521799325943\n",
      "Loss[459] = 0.3853406608104706\n",
      "Loss[460] = 0.385130912065506\n",
      "Loss[461] = 0.3849368691444397\n",
      "Loss[462] = 0.3847523629665375\n",
      "Loss[463] = 0.3845461308956146\n",
      "Loss[464] = 0.38434213399887085\n",
      "Loss[465] = 0.38416337966918945\n",
      "Loss[466] = 0.3839135468006134\n",
      "Loss[467] = 0.3838341534137726\n",
      "Loss[468] = 0.3837727904319763\n",
      "Loss[469] = 0.3835604190826416\n",
      "Loss[470] = 0.3833713233470917\n",
      "Loss[471] = 0.38321393728256226\n",
      "Loss[472] = 0.38300684094429016\n",
      "Loss[473] = 0.38269877433776855\n",
      "Loss[474] = 0.38255631923675537\n",
      "Loss[475] = 0.38235417008399963\n",
      "Loss[476] = 0.3820950984954834\n",
      "Loss[477] = 0.38192218542099\n",
      "Loss[478] = 0.3817020356655121\n",
      "Loss[479] = 0.3814918100833893\n",
      "Loss[480] = 0.38129961490631104\n",
      "Loss[481] = 0.38106152415275574\n",
      "Loss[482] = 0.380903959274292\n",
      "Loss[483] = 0.3807096481323242\n",
      "Loss[484] = 0.3805065155029297\n",
      "Loss[485] = 0.38033610582351685\n",
      "Loss[486] = 0.38010016083717346\n",
      "Loss[487] = 0.3799181580543518\n",
      "Loss[488] = 0.37968122959136963\n",
      "Loss[489] = 0.3795233368873596\n",
      "Loss[490] = 0.37934595346450806\n",
      "Loss[491] = 0.3791360855102539\n",
      "Loss[492] = 0.3789573609828949\n",
      "Loss[493] = 0.3787701427936554\n",
      "Loss[494] = 0.37855350971221924\n",
      "Loss[495] = 0.37834495306015015\n",
      "Loss[496] = 0.3781611919403076\n",
      "Loss[497] = 0.37795454263687134\n",
      "Loss[498] = 0.37778350710868835\n",
      "Loss[499] = 0.3776055574417114\n",
      "process took 10:22:31.537039 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "start = datetime.now()\n",
    "\n",
    "for step in range(500):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "     \n",
    "    print(f'Loss[{step}] = {loss_val}')\n",
    "        \n",
    "print(f'process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c16c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02866286  0.00391353 -0.01072783 ... -0.03264792  0.00024129\n",
      "  -0.00021848]\n",
      " [ 0.00095735 -0.0189361  -0.02946479 ...  0.04912312 -0.00165965\n",
      "   0.01241112]\n",
      " [ 0.03084684  0.04030455  0.01075094 ... -0.02086708 -0.00404731\n",
      "  -0.06132798]\n",
      " ...\n",
      " [-0.06701497  0.04543946  0.03028561 ... -0.01327508  0.04897631\n",
      "   0.03002343]\n",
      " [ 0.01275493  0.01944453 -0.04737942 ...  0.02383648 -0.04277041\n",
      "   0.06001806]\n",
      " [ 0.02494416 -0.00382685  0.0251975  ...  0.05030839 -0.0230723\n",
      "   0.03849551]]\n",
      "(927, 500)\n"
     ]
    }
   ],
   "source": [
    "#check param\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1601d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aafc0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_v_1 = jnp.array(pd.DataFrame(X_val_char).to_numpy())\n",
    "j_v_2 = jnp.array(pd.DataFrame(X_val_word).to_numpy())\n",
    "j_v_3 = jnp.array(pd.DataFrame(X_val_par).to_numpy())\n",
    "j_v_4 = jnp.array(pd.DataFrame(X_val_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137353, 78)\n"
     ]
    }
   ],
   "source": [
    "# print(dst_x.shape) #(412059, 960)\n",
    "y_pred = mainmodel.apply(params, j_v_1, j_v_2, j_v_3, j_v_4)\n",
    "print(y_pred.shape) #(137353, 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac1ea0",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a7197ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['county' 'genre' 'age' ... 'duration' 'class' 'jockey']\n",
      "(137353,)\n",
      "0.6771786681057324\n",
      "0.7096896318245688\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = helpers._proba_to_classes(y_pred, \"sherlock\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_validation, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55ad0a",
   "metadata": {},
   "source": [
    "## memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Submodel | Dropout separate key issue: Don't need to worry about. The aim is passing the same key to drop out same nodes across multiple devices.\n",
    "\n",
    "# Can't use one line here like how it's calling submodel in mainmodel?\n",
    "# https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.CallCompactUnboundModuleError\n",
    "# submodel = SubModel([300, 300], name='char_model')(j_1)\n",
    "submodel = SubModel([300, 300], name='char_model')\n",
    "\n",
    "\n",
    "#p_sub = submodel.init(jax.random.PRNGKey(0),jnp.ones((1, 960)))\n",
    "p_sub = submodel.init({'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(0)},jnp.ones((1, 960)))\n",
    "y = submodel.apply(p_sub, j_1, rngs={'dropout': jax.random.PRNGKey(0)})\n",
    "\n",
    "p_sub['params'].keys()\n",
    "\n",
    "# Do we need to use separate keys like - key1, key2, key3, key4 = random.split(random.PRNGKey(seed), 4) ?\n",
    "# https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_4_Flax_Zero2Hero_Colab.ipynb\n",
    "# https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.Dropout.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "067fbafb8497a6b213015d42733fcc9edf67b269b0a1726470d212d235a61220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
