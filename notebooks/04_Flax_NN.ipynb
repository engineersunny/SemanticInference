{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b7d583",
   "metadata": {},
   "source": [
    "#### simple nn example\n",
    "#### creates a 10-[12-8]-4 neural network.\n",
    "\n",
    "https://jamesmccaffrey.wordpress.com/2022/01/18/the-flax-neural-network-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d67f575",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    for feat in self.features[:-1]:\n",
    "      x = nn.relu(nn.Dense(feat)(x))\n",
    "    x = nn.Dense(self.features[-1])(x)\n",
    "    return x\n",
    "\n",
    "# creates a 10-[12-8]-4 neural network.\n",
    "model = MLP([12, 8, 4])\n",
    "batch = jnp.ones((32, 10))\n",
    "\n",
    "\n",
    "\n",
    "variables = model.init(jax.random.PRNGKey(0), batch)\n",
    "output = model.apply(variables, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357e884",
   "metadata": {},
   "source": [
    "#### LR w/ Flax\n",
    "https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/flax_basics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd493b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "\n",
    "# We create one dense layer instance (taking 'features' parameter as input)\n",
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4cea4",
   "metadata": {},
   "source": [
    "Layers (and models in general, we'll use that word from now on) are subclasses of the linen.Module class.\n",
    "\n",
    "Model parameters & initialization: \n",
    "Parameters are not stored with the models themselves. You need to initialize parameters by calling the init function, using a PRNGKey and a dummy input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dafc015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (10,)) # Dummy input\n",
    "#print(x.shape)\n",
    "\n",
    "params = model.init(key2, x) # Initialization call\n",
    "#print('\\n',params)\n",
    "\n",
    "jax.tree_map(lambda x: x.shape, params) # Checking output shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1874b17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 ,\n",
       "             -1.1271116 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3c5b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a pytree.\n",
    "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f369629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "def mse(params, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return jnp.inner(y-pred, y-pred) / 2.0\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47864f",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e790ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.02363979\n",
      "Loss step 0:  0.011577631\n",
      "Loss step 10:  0.011571459\n",
      "Loss step 20:  0.011569389\n",
      "Loss step 30:  0.011568717\n",
      "Loss step 40:  0.011568493\n",
      "Loss step 50:  0.011568409\n",
      "Loss step 60:  0.011568387\n",
      "Loss step 70:  0.011568378\n",
      "Loss step 80:  0.011568379\n",
      "Loss step 90:  0.011568367\n",
      "Loss step 100:  0.01156838\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "#same as the line above - loss_grad_fn = jax.value_and_grad(mse)\n",
    "@jax.value_and_grad\n",
    "def loss_grad_fn(params, x_batched, y_batched):\n",
    "  return mse(params, x_batched, y_batched)\n",
    "\n",
    "#also jax's way\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  params = update_params(params, learning_rate, grads)\n",
    "  if i % 10 == 0:\n",
    "    print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e03cd",
   "metadata": {},
   "source": [
    "### Optimizing with Optax\n",
    "\n",
    "Flax used to use its own `flax.optim` package for optimization, but with\n",
    "[FLIP #1009](https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md)\n",
    "this was deprecated in favor of\n",
    "[Optax](https://github.com/deepmind/optax).\n",
    "\n",
    "Basic usage of Optax is straightforward:\n",
    "\n",
    "1.   Choose an optimization method (e.g. `optax.sgd`).\n",
    "2.   Create optimizer state from parameters.\n",
    "3.   Compute the gradients of your loss with `jax.value_and_grad()`.\n",
    "4.   At every iteration, call the Optax `update` function to update the internal\n",
    "     optimizer state and create an update to the parameters. Then add the update\n",
    "     to the parameters with Optax's `apply_updates` method.\n",
    "\n",
    "Note that Optax can do a lot more: it's designed for composing simple gradient\n",
    "transformations into more complex transformations that allows to implement a\n",
    "wide range of optimizers. There is also support for changing optimizer\n",
    "hyperparameters over time (\"schedules\"), applying different updates to different\n",
    "parts of the parameter tree (\"masking\") and much more. For details please refer\n",
    "to the\n",
    "[official documentation](https://optax.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcf03a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msgd(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[43malpha\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (alpha)\n\u001b[1;32m      6\u001b[0m opt_state \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39minit(params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alpha' is not defined"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "    \n",
    "tx = optax.sgd(learning_rate=alpha)\n",
    "\n",
    "print (alpha)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2767e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mse() missing 2 required positional arguments: 'x_batched' and 'y_batched'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m   loss_val, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m   updates, opt_state \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m      4\u001b[0m   params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_jax_3/lib/python3.9/site-packages/jax/linear_util.py:166\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    170\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "\u001b[0;31mTypeError\u001b[0m: mse() missing 2 required positional arguments: 'x_batched' and 'y_batched'"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "  loss_val, grads = loss_grad_fn(params)\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(env_jax_3)",
   "language": "python",
   "name": "env_jax_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
