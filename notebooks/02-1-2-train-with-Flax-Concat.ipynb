{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 17:02:56.690610: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data process took 0:00:17.475816 seconds.\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "y_train = pd.read_parquet('../data/data/raw/train_labels.parquet').values.flatten()\n",
    "y_train = np.array([x.lower() for x in y_train])\n",
    "\n",
    "X_validation = pd.read_parquet('../data/data/processed/validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/data/raw/val_labels.parquet').values.flatten()\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/test.parquet')\n",
    "y_test = pd.read_parquet('../data/data/raw/test_labels.parquet').values.flatten()\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff95873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "from sherlock.deploy import helpers\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_train_char = X_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_validation[feature_cols[\"char\"]]\n",
    "X_val_word = X_validation[feature_cols[\"word\"]]\n",
    "X_val_par = X_validation[feature_cols[\"par\"]]\n",
    "X_val_rest = X_validation[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_train)   #(412059,)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(412059,78)\n",
    "y_val_int = encoder.transform(y_validation)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51913939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "960 single char level model training\n",
    "\n",
    "Dense_0: {\n",
    "    bias: (300,),\n",
    "    kernel: (960, 300),\n",
    "},\n",
    "Dense_1: {\n",
    "    bias: (300,),\n",
    "    kernel: (300, 300),\n",
    "},\n",
    "Dense_2: {\n",
    "    bias: (78,),\n",
    "    kernel: (300, 78),\n",
    "    \n",
    "1 layer + 2*300 layers + 78 output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee3435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "class SubModel(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    for feat in self.features[:-1]:\n",
    "      x = nn.relu(nn.Dense(feat)(x))\n",
    "    x = nn.Dense(self.features[-1])(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, concated_X):\n",
    "        #x = jnp.concatenate((x1, x2, x3, x4), axis=-1)\n",
    "        return nn.Dense(self.feature_size)(concated_X)\n",
    "\n",
    "    \n",
    "'''model = SubModel([300, 300, 78]) \n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng1, rng2 = jax.random.split(rng)\n",
    "params = model.init(rng2, jax.random.normal(rng1, (960,)))'''\n",
    "\n",
    "\n",
    "char_model_input = len(feature_cols[\"char\"])\n",
    "word_model_input = len(feature_cols[\"word\"])\n",
    "par_model_input = len(feature_cols[\"par\"])\n",
    "rest_model_input = len(feature_cols[\"rest\"])\n",
    "\n",
    "# Submodels to concat\n",
    "\n",
    "# layers: (960,300) (300,300)\n",
    "char_model = SubModel([300, 300])\n",
    "word_model = SubModel([200, 200])\n",
    "par_model = SubModel([400, 400])\n",
    "rest_model = SubModel([27])\n",
    "mainmodel = MainModel()\n",
    "\n",
    "# Dummy data to initialize\n",
    "b_char = jnp.ones((1, 960))\n",
    "b_word = jnp.ones((1, 201))\n",
    "b_par  = jnp.ones((1, 400))\n",
    "b_rest = jnp.ones((1, 27))\n",
    "\n",
    "# initialize dense layer\n",
    "p_char = char_model.init(jax.random.PRNGKey(0), b_char)\n",
    "p_word = word_model.init(jax.random.PRNGKey(0), b_word)\n",
    "p_par  = par_model.init(jax.random.PRNGKey(0), b_par)\n",
    "p_rest = rest_model.init(jax.random.PRNGKey(0), b_rest)\n",
    "\n",
    "j_r = jnp.concatenate((jnp.ones((1, 300)), jnp.ones((1, 200)), jnp.ones((1, 400)), jnp.ones((1, 27))), axis=-1)\n",
    "\n",
    "p_main = mainmodel.init(jax.random.PRNGKey(0),j_r) \n",
    "\n",
    "\n",
    "def main(x1, x2, x3, x4, p1, p2, p3, p4, pmain):\n",
    "    y1 = char_model.apply(p1, x1)\n",
    "    y2 = word_model.apply(p2, x2)\n",
    "    y3 = par_model.apply(p3, x3)\n",
    "    y4 = rest_model.apply(p4, x4)\n",
    "    \n",
    "    j_r = jnp.concatenate((y1,y2,y3,y4), axis=-1)\n",
    "    \n",
    "    return mainmodel.apply(pmain, j_r)\n",
    "\n",
    "lowered = jax.jit(main).lower(b_char, b_word, b_par, b_rest, p_char, p_word, p_par, p_rest, p_main)\n",
    "\n",
    "comp_dot = graphviz.Source(lowered._xla_computation().as_hlo_dot_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9c94ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn_outcome.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_dot.render('nn_outcome', view=True).replace('\\\\', '/') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf381d",
   "metadata": {},
   "source": [
    "OPTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func(x_batched, y_batched): \n",
    "  print(len(x_batched[0]))\n",
    "  def mse(p_main):    \n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x, y):      \n",
    "      pred = mainmodel.apply(p_main, x)\n",
    "      return jnp.inner(y-pred, y-pred) / 2.0  \n",
    "    \n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269f10e",
   "metadata": {},
   "source": [
    "Using training data:  df -> jnp\n",
    "\n",
    "http://bl.ocks.org/miguelusque/raw/f44a8e729896a96d0a3e4b07b5176af4/#numpy-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7f41a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"application/pdf\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588\n"
     ]
    }
   ],
   "source": [
    "params = p_main\n",
    "\n",
    "j_1 = jnp.array(pd.DataFrame(X_train_char).to_numpy())\n",
    "j_2 = jnp.array(pd.DataFrame(X_train_word).to_numpy())\n",
    "j_3 = jnp.array(pd.DataFrame(X_train_par).to_numpy())\n",
    "j_4 = jnp.array(pd.DataFrame(X_train_rest).to_numpy())\n",
    "\n",
    "dst_x = jnp.concatenate((j_1, j_2, j_3, j_4), axis=-1)\n",
    "j_r = jnp.array()\n",
    "\n",
    "\n",
    "dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "\n",
    "loss = make_mse_func(dst_x, dst_y)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ef69e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (653407623.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    len(dst_x[0]) 1588\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dst_x[0]\n",
    "len(dst_x[0]) #1588\n",
    "len(dst_x) #412059"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22217fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (927, 500), (1588, 500). (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Compute gradient of the loss.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     loss_val, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Update the optimiser state, create an update to the params.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 28 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mmake_mse_func.<locals>.mse\u001b[0;34m(p_main)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39minner(y\u001b[38;5;241m-\u001b[39mpred, y\u001b[38;5;241m-\u001b[39mpred) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m  \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Vectorise the squared error and compute the average of the loss.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquared_error\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batched\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mmake_mse_func.<locals>.mse.<locals>.squared_error\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquared_error\u001b[39m(x, y):      \n\u001b[0;32m---> 10\u001b[0m   pred \u001b[38;5;241m=\u001b[39m \u001b[43mmainmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39minner(y\u001b[38;5;241m-\u001b[39mpred, y\u001b[38;5;241m-\u001b[39mpred) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mMainModel.__call__\u001b[0;34m(self, concated_X)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, concated_X):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#x = jnp.concatenate((x1, x2, x3, x4), axis=-1)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcated_X\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_jax_3/lib/python3.9/site-packages/flax/linen/linear.py:183\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m  The transformed input.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m inputs \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 183\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m kernel \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(kernel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    188\u001b[0m y \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdot_general(inputs, kernel,\n\u001b[1;32m    189\u001b[0m                     (((inputs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (\u001b[38;5;241m0\u001b[39m,)), ((), ())),\n\u001b[1;32m    190\u001b[0m                     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_jax_3/lib/python3.9/site-packages/flax/core/scope.py:732\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, *init_args)\u001b[0m\n\u001b[1;32m    727\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 732\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text,\n\u001b[1;32m    733\u001b[0m           jnp\u001b[38;5;241m.\u001b[39mshape(val), jnp\u001b[38;5;241m.\u001b[39mshape(abs_val))\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (927, 500), (1588, 500). (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "start = datetime.now()\n",
    "\n",
    "for step in range(10):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if step%100 == 0: \n",
    "        print(f'Loss[{step}] = {loss_val}')\n",
    "        \n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c16c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check param\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training set\n",
    "model = mainmodel\n",
    "\n",
    "## predict\n",
    "print(dst_x.shape) #(412059, 960)\n",
    "y_pred = model.apply(params, dst_x)\n",
    "print(y.shape) #(412059, 78)\n",
    "\n",
    "## score\n",
    "y_pred_classes = helpers._proba_to_classes(y_pred, \"sherlock\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_train, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_train, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_char.shape) #(412059, 960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation set\n",
    "\n",
    "## predict\n",
    "print(X_val_char.shape) #(412059, 960)\n",
    "y_val = model.apply(params, X_val_char)\n",
    "print(y_val.shape) #(412059, 78)\n",
    "\n",
    "## score\n",
    "y_val_pred_classes = helpers._proba_to_classes(y_val, \"sherlock\")\n",
    "\n",
    "print(y_val_pred_classes)\n",
    "\n",
    "print(f1_score(y_validation, y_val_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_val_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1cab6",
   "metadata": {},
   "source": [
    "Other feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f02818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(env_jax_3)",
   "language": "python",
   "name": "env_jax_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
