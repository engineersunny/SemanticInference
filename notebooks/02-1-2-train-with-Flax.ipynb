{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 10:07:23.776520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data process took 0:00:17.063918 seconds.\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "y_train = pd.read_parquet('../data/data/raw/train_labels.parquet').values.flatten()\n",
    "y_train = np.array([x.lower() for x in y_train])\n",
    "\n",
    "X_validation = pd.read_parquet('../data/data/processed/validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/data/raw/val_labels.parquet').values.flatten()\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/test.parquet')\n",
    "y_test = pd.read_parquet('../data/data/raw/test_labels.parquet').values.flatten()\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff95873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "from sherlock.deploy import helpers\n",
    "\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_train_char = X_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_validation[feature_cols[\"char\"]]\n",
    "X_val_word = X_validation[feature_cols[\"word\"]]\n",
    "X_val_par = X_validation[feature_cols[\"par\"]]\n",
    "X_val_rest = X_validation[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_train)   #(412059,)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(412059,78)\n",
    "y_val_int = encoder.transform(y_validation)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24802e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e33dbd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = nn.Dense(features=1)\\n\\n#Model parameters & initialization\\nkey1, key2 = random.split(random.PRNGKey(0))\\nx = random.normal(key1, (3,)) # Dummy input\\nparams = model.init(key2, x) # Initialization call\\njax.tree_map(lambda x: x.shape, params) # Checking output shapes\\n\\n\\nchar_model_input, char_model = _build_char_submodel(X_train_char.shape[1]) #960\\nword_model_input, word_model = self._build_word_submodel(X_train_word.shape[1]) #200\\npar_model_input, par_model = self._build_par_submodel(X_train_par.shape[1])     #400\\nrest_model_input, rest_model = self._build_rest_submodel(X_train_rest.shape[1]) #27\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =================================================================================================\n",
    "#  _build_char_submodel with flax - line 241 from model.py\n",
    "\n",
    "'''\n",
    "https://flax.readthedocs.io/en/latest/flax.linen.html#module\n",
    "class Module(nn.Module):\n",
    "  features: Tuple[int] = (960, 200, 400, 27)\n",
    "\n",
    "  def setup(self):\n",
    "    self.dense1 = Dense(self.features[0])\n",
    "    self.dense2 = Dense(self.features[1])\n",
    "    self.dense3 = Dense(self.features[2])\n",
    "    self.dense4 = Dense(self.features[3])\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    combined2 = self.dense2(nn.relu(self.dense1(x)))\n",
    "    combined3 = self.dense3(nn.relu(combined2))\n",
    "    combined4 = self.dense3(nn.relu(combined3))\n",
    "    \n",
    "    return combined4\n",
    "'''\n",
    "\n",
    "#https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html\n",
    "'''char_model = nn.Dense(features=300)\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (960,)) # Dummy input\n",
    "params = char_model.init(key2, x) # Initialization call\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # Checking output shapes\n",
    "\n",
    "\n",
    "word_model = nn.Dense(features=200)\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (200,)) # Dummy input\n",
    "params = word_model.init(key2, x) # Initialization call\n",
    "print(jax.tree_map(lambda x: x.shape, params))\n",
    "\n",
    "\n",
    "par_model = nn.Dense(features=400)\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (400,)) # Dummy input\n",
    "params = par_model.init(key2, x) # Initialization call\n",
    "print(jax.tree_map(lambda x: x.shape, params))\n",
    "\n",
    "rest_model = nn.Dense(features=27)\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (27,)) # Dummy input\n",
    "params = rest_model.init(key2, x) # Initialization call\n",
    "print(jax.tree_map(lambda x: x.shape, params))'''\n",
    "\n",
    "#main - concat all todo - main.py line65\n",
    "\n",
    "# <=================================================================================================\n",
    "'''\n",
    "model = nn.Dense(features=1)\n",
    "\n",
    "#Model parameters & initialization\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (3,)) # Dummy input\n",
    "params = model.init(key2, x) # Initialization call\n",
    "jax.tree_map(lambda x: x.shape, params) # Checking output shapes\n",
    "\n",
    "\n",
    "char_model_input, char_model = _build_char_submodel(X_train_char.shape[1]) #960\n",
    "word_model_input, word_model = self._build_word_submodel(X_train_word.shape[1]) #200\n",
    "par_model_input, par_model = self._build_par_submodel(X_train_par.shape[1])     #400\n",
    "rest_model_input, rest_model = self._build_rest_submodel(X_train_rest.shape[1]) #27\n",
    "'''\n",
    "\n",
    "\n",
    "#todo 2 - merge model and build main network\n",
    "# model.py from line 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97d045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412059, 1588) (412059,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape) #df and np ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "960 single char level model training\n",
    "\n",
    "Dense_0: {\n",
    "    bias: (300,),\n",
    "    kernel: (960, 300),\n",
    "},\n",
    "Dense_1: {\n",
    "    bias: (300,),\n",
    "    kernel: (300, 300),\n",
    "},\n",
    "Dense_2: {\n",
    "    bias: (78,),\n",
    "    kernel: (300, 78),\n",
    "    \n",
    "1 layer + 2*300 layers + 78 output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee3435e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        Dense_0: {\n",
      "            kernel: DeviceArray([[ 0.06141227,  0.00766806, -0.05698649, ...,  0.01272237,\n",
      "                           0.01838974,  0.02176958],\n",
      "                         [-0.02200374, -0.07047574, -0.00778548, ..., -0.03661647,\n",
      "                          -0.01194724,  0.01908318],\n",
      "                         [-0.00935888,  0.00299105, -0.0068249 , ..., -0.05483543,\n",
      "                           0.03303373, -0.02801319],\n",
      "                         ...,\n",
      "                         [ 0.01294531, -0.00636607, -0.04631356, ...,  0.03514464,\n",
      "                          -0.00600298, -0.03478616],\n",
      "                         [-0.04506306,  0.03920445, -0.05522922, ..., -0.05057716,\n",
      "                          -0.01210736, -0.00505882],\n",
      "                         [ 0.05229904, -0.02613135, -0.0082468 , ..., -0.04306015,\n",
      "                           0.00207464, -0.04712147]], dtype=float32),\n",
      "            bias: DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            dtype=float32),\n",
      "        },\n",
      "        Dense_1: {\n",
      "            kernel: DeviceArray([[-0.0395382 , -0.05079179, -0.04458412, ..., -0.05948203,\n",
      "                          -0.02535578,  0.08782101],\n",
      "                         [ 0.08696356,  0.01874232, -0.09610173, ..., -0.02200339,\n",
      "                           0.04373693, -0.03806578],\n",
      "                         [-0.03698347, -0.03600912, -0.08756911, ...,  0.02114769,\n",
      "                           0.04105579,  0.02234525],\n",
      "                         ...,\n",
      "                         [ 0.0118073 ,  0.01242179,  0.02121691, ...,  0.09666303,\n",
      "                           0.04885739,  0.09218276],\n",
      "                         [-0.04749037, -0.07200161, -0.11643427, ...,  0.00978421,\n",
      "                           0.04775093,  0.0115128 ],\n",
      "                         [-0.00660479,  0.10263914,  0.02781452, ...,  0.09321827,\n",
      "                          -0.04063657, -0.04097414]], dtype=float32),\n",
      "            bias: DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],            dtype=float32),\n",
      "        },\n",
      "        Dense_2: {\n",
      "            kernel: DeviceArray([[-0.02269645,  0.05840167,  0.00228389, ..., -0.03671095,\n",
      "                           0.05301209,  0.0101417 ],\n",
      "                         [-0.01754842, -0.01749577, -0.00855719, ..., -0.00028081,\n",
      "                          -0.04985965, -0.03518622],\n",
      "                         [ 0.04078916, -0.04627252,  0.03881517, ...,  0.07886171,\n",
      "                          -0.02120088, -0.02012892],\n",
      "                         ...,\n",
      "                         [ 0.0347394 ,  0.03904321,  0.06361676, ..., -0.11977095,\n",
      "                          -0.04346298, -0.06804616],\n",
      "                         [-0.02994283, -0.04450368, -0.01262212, ...,  0.08515811,\n",
      "                          -0.02805004, -0.06076699],\n",
      "                         [-0.12136894,  0.0065161 ,  0.12810841, ...,  0.06836823,\n",
      "                          -0.02079089, -0.00977013]], dtype=float32),\n",
      "            bias: DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                         0., 0., 0.], dtype=float32),\n",
      "        },\n",
      "    },\n",
      "})\n",
      "(412059, 960)\n",
      "(412059, 78)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        Dense_0: {\n",
       "            bias: (300,),\n",
       "            kernel: (960, 300),\n",
       "        },\n",
       "        Dense_1: {\n",
       "            bias: (300,),\n",
       "            kernel: (300, 300),\n",
       "        },\n",
       "        Dense_2: {\n",
       "            bias: (78,),\n",
       "            kernel: (300, 78),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try 960 model only\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    for feat in self.features[:-1]:\n",
    "      x = nn.relu(nn.Dense(feat)(x))\n",
    "    x = nn.Dense(self.features[-1])(x) # ?\n",
    "    return x\n",
    "\n",
    "model = MLP([300, 300, 78]) # do we need 78 layer for model concat..?\n",
    "#dummy_input = jnp.ones((412059, 960))\n",
    "#params = model.init(jax.random.PRNGKey(0), dummy_input)\n",
    "\n",
    "\n",
    "#==================================\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng1, rng2 = jax.random.split(rng)\n",
    "params = model.init(rng2, jax.random.normal(rng1, (960,)))\n",
    "print(params)\n",
    "\n",
    "\n",
    "## predict\n",
    "print(X_train_char.shape) #(412059, 960)\n",
    "y = model.apply(params, X_train_char)\n",
    "print(y.shape) #(412059, 78)\n",
    "\n",
    "jax.tree_map(lambda dummy_input: dummy_input.shape, params) # Checking output shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfdac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06141227  0.00766806 -0.05698649 ...  0.01272237  0.01838974\n",
      "   0.02176958]\n",
      " [-0.02200374 -0.07047574 -0.00778548 ... -0.03661647 -0.01194724\n",
      "   0.01908318]\n",
      " [-0.00935888  0.00299105 -0.0068249  ... -0.05483543  0.03303373\n",
      "  -0.02801319]\n",
      " ...\n",
      " [ 0.01294531 -0.00636607 -0.04631356 ...  0.03514464 -0.00600298\n",
      "  -0.03478616]\n",
      " [-0.04506306  0.03920445 -0.05522922 ... -0.05057716 -0.01210736\n",
      "  -0.00505882]\n",
      " [ 0.05229904 -0.02613135 -0.0082468  ... -0.04306015  0.00207464\n",
      "  -0.04712147]]\n",
      "(960, 300)\n"
     ]
    }
   ],
   "source": [
    "#print(params)\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "\n",
    "\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661a905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4078da3",
   "metadata": {},
   "source": [
    "### Optimizing with Optax\n",
    "\n",
    "Flax used to use its own `flax.optim` package for optimization, but with\n",
    "[FLIP #1009](https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md)\n",
    "this was deprecated in favor of\n",
    "[Optax](https://github.com/deepmind/optax).\n",
    "\n",
    "Basic usage of Optax is straightforward:\n",
    "\n",
    "1.   Choose an optimization method (e.g. `optax.sgd`).\n",
    "2.   Create optimizer state from parameters.\n",
    "3.   Compute the gradients of your loss with `jax.value_and_grad()`.\n",
    "4.   At every iteration, call the Optax `update` function to update the internal\n",
    "     optimizer state and create an update to the parameters. Then add the update\n",
    "     to the parameters with Optax's `apply_updates` method.\n",
    "\n",
    "Note that Optax can do a lot more: it's designed for composing simple gradient\n",
    "transformations into more complex transformations that allows to implement a\n",
    "wide range of optimizers. There is also support for changing optimizer\n",
    "hyperparameters over time (\"schedules\"), applying different updates to different\n",
    "parts of the parameter tree (\"masking\") and much more. For details please refer\n",
    "to the\n",
    "[official documentation](https://optax.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(params)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func(x_batched, y_batched):\n",
    "  def mse(params):    \n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x, y):      \n",
    "      pred = model.apply(params, x)\n",
    "      print(\"y and y pred\")\n",
    "      print(y[0])\n",
    "      print(pred[0])\n",
    "    \n",
    "      return jnp.inner(y-pred, y-pred) / 2.0\n",
    "    \n",
    "    print(\"x and y batched\")\n",
    "    print(x_batched.shape)\n",
    "    print(y_batched.shape)\n",
    "    \n",
    "    \n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3a056",
   "metadata": {},
   "source": [
    "ver1. Using dummy sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59628a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.051781\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "nsamples = 20\n",
    "xdim = 960\n",
    "ydim = 78\n",
    "\n",
    "# Generate random ground truth w and b.\n",
    "w = jax.random.normal(rng1, (xdim, ydim))\n",
    "b = jax.random.normal(rng2, (ydim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "ksample, knoise = jax.random.split(rng1)\n",
    "x_samples = jax.random.normal(ksample, (nsamples, xdim))\n",
    "y_samples = jnp.dot(x_samples, w) + b\n",
    "y_samples += 0.1 * jax.random.normal(knoise, (nsamples, ydim))\n",
    "\n",
    "\n",
    "#================================================\n",
    "\n",
    "print(y_samples[0][0])\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "loss = make_mse_func(x_samples, y_samples)\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b014469d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y batched\n",
      "(20, 960)\n",
      "(20, 78)\n",
      "y and y pred\n",
      "Traced<ShapedArray(float32[])>with<BatchTrace(level=3/1)> with\n",
      "  val = Traced<ShapedArray(float32[20])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  batch_dim = 0\n",
      "Traced<ShapedArray(float32[])>with<BatchTrace(level=3/1)> with\n",
      "  val = Traced<ShapedArray(float32[20])>with<JVPTrace(level=2/1)> with\n",
      "    primal = Traced<ShapedArray(float32[20])>with<DynamicJaxprTrace(level=0/1)>\n",
      "    tangent = Traced<ShapedArray(float32[20])>with<JaxprTrace(level=1/1)> with\n",
      "      pval = (ShapedArray(float32[20]), *)\n",
      "      recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fe696069c70>, invars=(Traced<ShapedArray(float32[20,78]):JaxprTrace(level=1/1)>, Traced<ShapedArray(int32[1]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fe68ec11400; to 'JaxprTracer' at 0x7fe68ec113b0>], primitive=gather, params={'dimension_numbers': GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,)), 'slice_sizes': (20, 1), 'unique_indices': True, 'indices_are_sorted': True, 'mode': <GatherScatterMode.PROMISE_IN_BOUNDS: 3>, 'fill_value': None}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fe68e4daeb0>, name_stack=NameStack(stack=(Transform(name='vmap'),))))\n",
      "  batch_dim = 0\n",
      "Loss[0] = 38539.15234375\n",
      "Loss[10] = 2409.217529296875\n",
      "Loss[20] = 575.9647827148438\n",
      "Loss[30] = 194.80775451660156\n",
      "Loss[40] = 73.82444763183594\n",
      "Loss[50] = 25.7294864654541\n",
      "Loss[60] = 9.704497337341309\n",
      "Loss[70] = 3.3531346321105957\n",
      "Loss[80] = 1.1586612462997437\n",
      "Loss[90] = 0.4371238350868225\n"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "for step in range(n_training_steps):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if step%10 == 0: print(f'Loss[{step}] = {loss_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269f10e",
   "metadata": {},
   "source": [
    "ver2. Using training data:  df -> jnp\n",
    "\n",
    "http://bl.ocks.org/miguelusque/raw/f44a8e729896a96d0a3e4b07b5176af4/#numpy-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d7f41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy = pd.DataFrame(X_train_char).to_numpy()\n",
    "dst_x = jnp.array(npy)\n",
    "dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "loss = make_mse_func(dst_x, dst_y)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "ver2. Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22217fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y batched\n",
      "(412059, 960)\n",
      "(412059, 78)\n",
      "y and y pred\n",
      "Traced<ShapedArray(float32[])>with<BatchTrace(level=3/1)> with\n",
      "  val = Traced<ShapedArray(float32[412059])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  batch_dim = 0\n",
      "Traced<ShapedArray(float32[])>with<BatchTrace(level=3/1)> with\n",
      "  val = Traced<ShapedArray(float32[412059])>with<JVPTrace(level=2/1)> with\n",
      "    primal = Traced<ShapedArray(float32[412059])>with<DynamicJaxprTrace(level=0/1)>\n",
      "    tangent = Traced<ShapedArray(float32[412059])>with<JaxprTrace(level=1/1)> with\n",
      "      pval = (ShapedArray(float32[412059]), *)\n",
      "      recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fe6745d76a0>, invars=(Traced<ShapedArray(float32[412059,78]):JaxprTrace(level=1/1)>, Traced<ShapedArray(int32[1]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fe6741ddae0; to 'JaxprTracer' at 0x7fe6741dda90>], primitive=gather, params={'dimension_numbers': GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,)), 'slice_sizes': (412059, 1), 'unique_indices': True, 'indices_are_sorted': True, 'mode': <GatherScatterMode.PROMISE_IN_BOUNDS: 3>, 'fill_value': None}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fe6741c18b0>, name_stack=NameStack(stack=(Transform(name='vmap'),))))\n",
      "  batch_dim = 0\n",
      "Loss[0] = 23108.873046875\n",
      "Load data process took 0:13:11.313598 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "start = datetime.now()\n",
    "\n",
    "for step in range(100):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if step%100 == 0: \n",
    "        print(f'Loss[{step}] = {loss_val}')\n",
    "        \n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c16c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12023748 -0.10125136  0.03266515 ...  0.02326848 -0.00838813\n",
      "  -0.1158741 ]\n",
      " [ 0.02102338 -0.03948994  0.02134902 ... -0.03727025 -0.07971643\n",
      "  -0.00752137]\n",
      " [ 0.04864246 -0.02335854  0.13749954 ... -0.10765366  0.04497682\n",
      "  -0.08829252]\n",
      " ...\n",
      " [-0.06597637 -0.06548307 -0.05338138 ... -0.07340642 -0.11059001\n",
      "  -0.12669243]\n",
      " [-0.09226173  0.04365811  0.06232785 ... -0.05720239 -0.05338946\n",
      "  -0.16088803]\n",
      " [ 0.10310815 -0.06137793  0.05642555 ... -0.15567093 -0.0634543\n",
      "  -0.04840209]]\n",
      "(960, 300)\n"
     ]
    }
   ],
   "source": [
    "#check param\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412059, 960)\n",
      "(412059, 78)\n",
      "['jockey' 'jockey' 'jockey' ... 'jockey' 'jockey' 'jockey']\n",
      "(412059,)\n",
      "0.0008584845270142998\n",
      "0.02093389538876714\n"
     ]
    }
   ],
   "source": [
    "## Training set\n",
    "\n",
    "## predict\n",
    "print(X_train_char.shape) #(412059, 960)\n",
    "y_pred = model.apply(params, X_train_char)\n",
    "print(y.shape) #(412059, 78)\n",
    "\n",
    "## score\n",
    "y_pred_classes = helpers._proba_to_classes(y_pred, \"sherlock\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_train, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_train, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344c61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137353, 960)\n",
      "(137353, 78)\n",
      "['jockey' 'jockey' 'jockey' ... 'jockey' 'jockey' 'jockey']\n",
      "0.0008243477687499784\n",
      "0.020509198925396606\n"
     ]
    }
   ],
   "source": [
    "## Validation set\n",
    "\n",
    "## predict\n",
    "print(X_val_char.shape) #(412059, 960)\n",
    "y_val = model.apply(params, X_val_char)\n",
    "print(y_val.shape) #(412059, 78)\n",
    "\n",
    "## score\n",
    "y_val_pred_classes = helpers._proba_to_classes(y_val, \"sherlock\")\n",
    "\n",
    "print(y_val_pred_classes)\n",
    "\n",
    "print(f1_score(y_validation, y_val_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_val_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1cab6",
   "metadata": {},
   "source": [
    "Other feature models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14f02818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 8), dtype=float32, numpy=\n",
       "array([[ 0.07428205, -0.0114342 , -0.43905738, -0.3311604 ,  0.02759314,\n",
       "         0.47437763, -0.42301235,  0.55930805],\n",
       "       [ 0.1284771 , -1.1319422 , -2.542329  , -1.7167022 , -0.81129485,\n",
       "         0.8984709 , -0.36999488,  1.3511837 ],\n",
       "       [ 0.18267214, -2.25245   , -4.645601  , -3.1022441 , -1.6501828 ,\n",
       "         1.3225641 , -0.3169775 ,  2.1430593 ],\n",
       "       [ 0.23686719, -3.3729582 , -6.7488728 , -4.4877863 , -2.489071  ,\n",
       "         1.7466574 , -0.26396012,  2.9349349 ],\n",
       "       [ 0.29106224, -4.493466  , -8.852144  , -5.8733277 , -3.3279588 ,\n",
       "         2.1707506 , -0.2109425 ,  3.7268105 ]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    concatenate,\n",
    ")\n",
    "input_ = np.arange(10).reshape(5, 2)\n",
    "print(input_)\n",
    "\n",
    "\n",
    "x1 = tf.keras.layers.Dense(8)(input_)\n",
    "x2 = tf.keras.layers.Dense(8)(input_)\n",
    "\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe376599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name='input_9'), name='input_9', description=\"created by layer 'input_9'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 16), dtype=tf.float32, name=None), name='dense_22/Softmax:0', description=\"created by layer 'dense_22'\")\n",
      "<keras.engine.functional.Functional object at 0x7fe6741f9bb0>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, model_from_json\n",
    "\n",
    "x = Input(shape=(32,))\n",
    "y = Dense(16, activation='softmax')(x)\n",
    "model = Model(x, y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10d51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import sklearn, numpy\n",
    "\n",
    "T = numpy.array(\n",
    "  [[1,2,3],    [4,5,6],    [7,8,9]],\n",
    "\n",
    "  )\n",
    "print(T.shape)\n",
    "print(T)\n",
    "print(T[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(env_jax_3)",
   "language": "python",
   "name": "env_jax_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
