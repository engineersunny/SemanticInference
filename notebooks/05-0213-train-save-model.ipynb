{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "2023-03-21 11:06:08.855508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 11:06:08.855622: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 11:06:08.855634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.deploy import helpers\n",
    "import graphviz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "jax.devices()\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data process took 0:00:02.114853 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "\n",
    "y_train = pd.read_parquet('../data/data/raw/train_labels.parquet').values.flatten()\n",
    "y_train = np.array([x.lower() for x in y_train])\n",
    "\n",
    "X_validation = pd.read_parquet('../data/data/processed/validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/data/raw/val_labels.parquet').values.flatten()\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/test.parquet')\n",
    "y_test = pd.read_parquet('../data/data/raw/test_labels.parquet').values.flatten()\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff95873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(set(y_train))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_train_char = X_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_validation[feature_cols[\"char\"]]\n",
    "X_val_word = X_validation[feature_cols[\"word\"]]\n",
    "X_val_par = X_validation[feature_cols[\"par\"]]\n",
    "X_val_rest = X_validation[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_train)   #(412059,)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(412059,78)\n",
    "\n",
    "y_val_int = encoder.transform(y_validation)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e93aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(pd.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train     # labels #(412059,)\n",
    "y_train_int # numeric labels #(412059,)\n",
    "y_train_cat # (412059,78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afa8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loads data onto gpu\n",
    "# j_1 = jnp.array(pd.DataFrame(X_train_char).to_numpy())\n",
    "# j_2 = jnp.array(pd.DataFrame(X_train_word).to_numpy())\n",
    "# j_3 = jnp.array(pd.DataFrame(X_train_par).to_numpy())\n",
    "# j_4 = jnp.array(pd.DataFrame(X_train_rest).to_numpy())\n",
    "\n",
    "#j_2.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cb2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j_1.shape 960\n",
    "# j_2.shape 201\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33df66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "  \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4f00e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    training: bool = True\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # batchnormalisation - https://github.com/google/flax/issues/932\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        x = nn.relu(nn.Dense(self.features[0])(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "                \n",
    "        x = nn.relu(nn.Dense(self.features[1])(x)) \n",
    "        \n",
    "        # todo: add \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b45c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[4146024105  967050713]\n",
      "[0.14389051]\n",
      "[-2.6105583   0.03385283  1.0863333  -1.480299    0.48895672  1.062516\n",
      "  0.54174834  0.0170228   0.2722685   0.30522448]\n"
     ]
    }
   ],
   "source": [
    "# PRNGKey Example\n",
    "print(random.PRNGKey(0))\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "print(key1)\n",
    "print(random.normal(key1,shape=(1,)))\n",
    "\n",
    "a = random.normal(key1, (10,))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee3435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    num_classes: int = 78\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x1, x2, x3, x4):\n",
    "       \n",
    "        # [1] define shape        \n",
    "        y1 = SubModel([300, 300], name='char_model')(x1)      \n",
    "        y2 = SubModel([200, 200], name='word_model')(x2)\n",
    "        y3 = SubModel([400, 400], name='par_model')(x3)\n",
    "        y4 = RestModel([27], name='rest_model')(x4)\n",
    "                      \n",
    "        # [2] concat submodels    \n",
    "        x = jnp.concatenate((y1, y2, y3, y4), axis=-1)\n",
    "        \n",
    "        print(\"check mainmodel shape\")\n",
    "        print(np.shape(x))\n",
    "        \n",
    "        # batchnormalisation\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        # dense 1\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "        \n",
    "        # dense 2\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dense w/ softmax - todo: check\n",
    "        x = nn.softmax(nn.Dense(self.feature_size)(x), axis=-1)\n",
    "        \n",
    "        return nn.Dense(self.num_classes)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590cbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "mainmodel = MainModel()\n",
    "p_main = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ffb6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['char_model', 'word_model', 'par_model', 'rest_model', 'BatchNorm_0', 'Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p_main\n",
    "p_main['params'].keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbbf381d",
   "metadata": {},
   "source": [
    "## Training - using trainstate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e0a2a2",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb64e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "PRNGKey = Any\n",
    "Dataset = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d6f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = {\n",
    "    'char': pd.DataFrame(X_train_char).to_numpy(),\n",
    "    'word': pd.DataFrame(X_train_word).to_numpy(),\n",
    "    'par': pd.DataFrame(X_train_par).to_numpy(),\n",
    "    'test': pd.DataFrame(X_train_rest).to_numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d9c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_collator(rng: PRNGKey, \n",
    "                        char_ds, word_ds, par_ds, rest_ds,\n",
    "                        labels, \n",
    "                        batch_size: int):\n",
    "    len_dataset = len(char_ds)\n",
    "    steps_per_epoch = len_dataset // batch_size\n",
    "    perms = jax.random.permutation(rng, len_dataset)\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size)) \n",
    "    \n",
    "    for perm in perms:\n",
    "        batch = {\n",
    "            'char': char_ds[perm],\n",
    "            'word': word_ds[perm],\n",
    "            'par': par_ds[perm],\n",
    "            'rest': rest_ds[perm],\n",
    "            'labels': labels[perm]\n",
    "        }\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5226a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = jax.random.PRNGKey(123)\n",
    "# rng, sample_rng = jax.random.split(rng)\n",
    "\n",
    "\n",
    "# #dst_x = jnp.concatenate((j_1, j_2, j_3, j_4), axis=-1)\n",
    "# #dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# train_data_loader = train_data_collator(\n",
    "#     sample_rng,\n",
    "#     pd.DataFrame(X_train_char).to_numpy(),\n",
    "#     pd.DataFrame(X_train_word).to_numpy(),\n",
    "#     pd.DataFrame(X_train_par).to_numpy(),\n",
    "#     pd.DataFrame(X_train_rest).to_numpy(),\n",
    "#     y_train_cat,\n",
    "#     128\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "195e6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_data_loader))\n",
    "# batch['char'].shape\n",
    "\n",
    "# batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html\n",
    "#https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/examples/mnist/train.py#L109\n",
    "from flax.training import train_state \n",
    "\n",
    "learning_rate = 0.0001\n",
    "#n_training_steps = 100\n",
    "\n",
    "def create_train_state(rng):\n",
    "  mainmodel = MainModel()\n",
    "  params = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27)))\n",
    "  tx=optax.adam(learning_rate=learning_rate)\n",
    "  state = train_state.TrainState.create(apply_fn=mainmodel.apply,\n",
    "                                        params=params,\n",
    "                                        tx=tx)\n",
    "  return state \n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, batch):\n",
    "  \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "  \n",
    "  def loss_fn(params):\n",
    "    logits = state.apply_fn(params, batch['char'],batch['word'], batch['par'], batch['rest'],) #{'params': params}\n",
    "    \n",
    "    #one_hot = jax.nn.one_hot(label, 6)    \n",
    "    #xentropy\n",
    "    entropy = optax.softmax_cross_entropy(logits=logits, labels = batch['labels'])\n",
    "    loss = jnp.mean(entropy)\n",
    "    \n",
    "    return loss, logits\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  \n",
    "  (loss, logits), grads = grad_fn(state.params)\n",
    "  \n",
    "  #accuracy = jnp.mean(jnp.argmax(logits, -1) == label)\n",
    "  \n",
    "  \n",
    "  return grads, loss #, accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "def train_epoch(state, batch, rng):\n",
    "  grads, loss = apply_model(state, batch) #, accuracy   -- Entry Parameter Subshape: f32[412059,78]\n",
    "  state = update_model(state, grads)  \n",
    "  return state, loss #, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a0f86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "\n",
    "#     feature_size = 500\n",
    "#     num_classes = 78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22217fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "# def train_and_evaluate\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "state = create_train_state(init_rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47b54188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(2048, 927)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:06:30.733682: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.3 = f32[2048,300]{1,0} custom-call(f32[2048,960]{1,0} %add.24, f32[960,300]{1,0} %Arg_23.24, f32[2048,300]{1,0} %broadcast.54), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/char_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:30.775372: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.7 = f32[2048,300]{1,0} custom-call(f32[2048,300]{1,0} %maximum.25, f32[300,300]{1,0} %Arg_25.26, f32[2048,300]{1,0} %broadcast.58), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/char_model/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:30.803034: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.11 = f32[2048,200]{1,0} custom-call(f32[2048,201]{1,0} %add.26, f32[201,200]{1,0} %Arg_37.38, f32[2048,200]{1,0} %broadcast.68), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/word_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:30.830696: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.15 = f32[2048,200]{1,0} custom-call(f32[2048,200]{1,0} %maximum.27, f32[200,200]{1,0} %Arg_39.40, f32[2048,200]{1,0} %broadcast.72), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/word_model/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:30.885859: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.19 = f32[2048,400]{1,0} custom-call(f32[2048,400]{1,0} %add.25, f32[400,400]{1,0} %Arg_29.30, f32[2048,400]{1,0} %broadcast.80), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/par_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:30.957530: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.27 = f32[2048,500]{1,0} custom-call(f32[2048,927]{1,0} %add.21, f32[927,500]{1,0} %Arg_13.14, f32[2048,500]{1,0} %broadcast.94), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.026863: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.31 = f32[2048,500]{1,0} custom-call(f32[2048,500]{1,0} %maximum.23, f32[500,500]{1,0} %Arg_15.16, f32[2048,500]{1,0} %broadcast.99), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=33}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.039420: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.39 = f32[2048,78]{1,0} custom-call(f32[2048,500]{1,0} %divide.14, f32[500,78]{1,0} %Arg_19.20, f32[2048,78]{1,0} %broadcast.109), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(apply_model)/jit(main)/jvp(MainModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=38}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.041584: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.41 = f32[2048,500]{1,0} custom-call(f32[2048,78]{1,0} %add.23, f32[500,78]{1,0} %Arg_19.20), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=38}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.045467: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.43 = f32[2048,500]{1,0} custom-call(f32[2048,500]{1,0} %multiply.50, f32[500,500]{1,0} %Arg_17.18), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=36}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.050943: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.47 = f32[2048,927]{1,0} custom-call(f32[2048,500]{1,0} %select.442, f32[927,500]{1,0} %Arg_13.14), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.053304: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.49 = f32[2048,300]{1,0} custom-call(f32[2048,300]{1,0} %select.605, f32[300,300]{1,0} %Arg_25.26), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/char_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.057987: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.51 = f32[2048,960]{1,0} custom-call(f32[2048,300]{1,0} %select.614, f32[960,300]{1,0} %Arg_23.24), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/char_model/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.061091: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.53 = f32[2048,400]{1,0} custom-call(f32[2048,400]{1,0} %select.511, f32[400,400]{1,0} %Arg_31.32), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/par_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.062733: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.57 = f32[2048,200]{1,0} custom-call(f32[2048,200]{1,0} %select.558, f32[200,200]{1,0} %Arg_39.40), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/word_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.064424: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.59 = f32[2048,201]{1,0} custom-call(f32[2048,200]{1,0} %select.567, f32[201,200]{1,0} %Arg_37.38), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/word_model/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.069848: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.61 = f32[927,500]{1,0} custom-call(f32[2048,927]{1,0} %add.21, f32[2048,500]{1,0} %select.442), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=27}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.073719: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.63 = f32[500,500]{1,0} custom-call(f32[2048,500]{1,0} %maximum.23, f32[2048,500]{1,0} %select.433), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=33}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.076051: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.67 = f32[500,78]{1,0} custom-call(f32[2048,500]{1,0} %divide.14, f32[2048,78]{1,0} %add.23), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/Dense_3/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/232481509.py\" source_line=38}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.080702: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.69 = f32[960,300]{1,0} custom-call(f32[2048,960]{1,0} %add.24, f32[2048,300]{1,0} %select.614), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/char_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.083069: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.71 = f32[300,300]{1,0} custom-call(f32[2048,300]{1,0} %maximum.25, f32[2048,300]{1,0} %select.605), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/char_model/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.086240: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.73 = f32[400,400]{1,0} custom-call(f32[2048,400]{1,0} %add.25, f32[2048,400]{1,0} %select.520), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/par_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.087998: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.77 = f32[201,200]{1,0} custom-call(f32[2048,201]{1,0} %add.26, f32[2048,200]{1,0} %select.567), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/word_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 11:06:31.089786: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.79 = f32[200,200]{1,0} custom-call(f32[2048,200]{1,0} %maximum.27, f32[2048,200]{1,0} %select.558), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(apply_model)/jit(main)/transpose(jvp(MainModel))/word_model/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1452975/4269309059.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[10] = 3.937894582748413\n",
      "Loss[20] = 3.6082451343536377\n",
      "Loss[30] = 3.3790738582611084\n",
      "Loss[40] = 3.1330316066741943\n",
      "Loss[50] = 2.9308037757873535\n",
      "Loss[60] = 2.7597804069519043\n",
      "Loss[70] = 2.626626491546631\n",
      "Loss[80] = 2.513958215713501\n",
      "Loss[90] = 2.439659833908081\n",
      "Loss[100] = 2.3724422454833984\n"
     ]
    }
   ],
   "source": [
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "\n",
    "for epoch in range(1, 100 + 1):\n",
    "  \n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "  \n",
    "  train_data_loader = train_data_collator(\n",
    "        input_rng,\n",
    "        pd.DataFrame(X_train_char).to_numpy(),\n",
    "        pd.DataFrame(X_train_word).to_numpy(),\n",
    "        pd.DataFrame(X_train_par).to_numpy(),\n",
    "        pd.DataFrame(X_train_rest).to_numpy(),\n",
    "        onehot(y_train_int, num_labels),#dst_y,\n",
    "        2048\n",
    "    )\n",
    "  \n",
    "  \n",
    "  for batch in train_data_loader:\n",
    "    #state, train_loss = train_epoch(state, batch, input_rng)\n",
    "    state, train_loss = train_epoch(state, batch,input_rng)\n",
    "  \n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'Loss[{epoch}] = {train_loss}') #, accuracy = {accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "669da501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412059"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2db9275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "### TO BYTE | FROM BYTE (more for custom use..?)\n",
    "\n",
    "## SAVE\n",
    "## TrainState -> bytes -  file\n",
    "data = flax.serialization.to_bytes(state)\n",
    "with open(\"flax_model_MAR.msgpack\", \"wb\") as binary_file:\n",
    "    binary_file.write(data)\n",
    "       \n",
    "## RESTORE\n",
    "# file - byte -> TrainState\n",
    "with open(\"flax_model_MAR.msgpack\", mode='rb') as file:  # b is important -> binary\n",
    "    read_data = file.read()\n",
    "\n",
    "new_state = create_train_state(init_rng)\n",
    "restored_state = flax.serialization.from_bytes(new_state, read_data)  ##TrainState\n",
    "\n",
    "assert jax.tree_util.tree_all(jax.tree_map(lambda x, y: (x == y).all(), state.params, restored_state.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c16c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check param\n",
    "# print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "# print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b460981f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b1601d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aafc0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_v_1 = jnp.array(pd.DataFrame(X_val_char).to_numpy())\n",
    "j_v_2 = jnp.array(pd.DataFrame(X_val_word).to_numpy())\n",
    "j_v_3 = jnp.array(pd.DataFrame(X_val_par).to_numpy())\n",
    "j_v_4 = jnp.array(pd.DataFrame(X_val_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(dst_x.shape) #(412059, 960)\n",
    "# y_pred = mainmodel.apply(params, j_v_1, j_v_2, j_v_3, j_v_4)\n",
    "# print(y_pred.shape) #(137353, 78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b3da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(137353, 927)\n",
      "check mainmodel shape\n",
      "(137353, 927)\n"
     ]
    }
   ],
   "source": [
    "y_pred = mainmodel.apply(state.params, j_v_1, j_v_2, j_v_3, j_v_4)\n",
    "y_pred_sv = mainmodel.apply(restored_state.params, j_v_1, j_v_2, j_v_3, j_v_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac1ea0",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a7197ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['county' 'category' 'rank' ... 'county' 'sex' 'city']\n",
      "(137353,)\n",
      "0.1074678231830563\n",
      "0.2165151107001667\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = helpers._proba_to_classes(y_pred_sv, \"sherlock\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_validation, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aefecb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['county' 'collection' 'age' ... 'duration' 'class' 'jockey']\n",
      "['county' 'category' 'rank' ... 'county' 'sex' 'city']\n"
     ]
    }
   ],
   "source": [
    "print(y_validation)\n",
    "print(y_pred_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherlock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
