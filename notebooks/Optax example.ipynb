{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98097c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/deepmind/optax/blob/master/examples/flax_example.py\n",
    "\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4f44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        kernel: DeviceArray([[ 2.35571519e-01, -1.71652585e-01, -4.45728786e-02,\n",
      "                      -4.68043566e-01,  4.54595268e-01],\n",
      "                     [-6.87736452e-01,  3.67835373e-01, -1.79262087e-01,\n",
      "                       1.29276231e-01, -2.42580160e-01],\n",
      "                     [ 2.02303097e-01, -2.49465615e-01,  2.74955630e-01,\n",
      "                       4.73488361e-01, -1.98002517e-01],\n",
      "                     [ 2.74478316e-01, -1.21369645e-01, -2.25361675e-01,\n",
      "                      -4.78193641e-01, -9.63979885e-02],\n",
      "                     [-6.19886033e-02, -1.72743499e-01,  2.96945305e-04,\n",
      "                      -7.17593372e-01,  2.00894207e-01],\n",
      "                     [-5.60321152e-01,  3.27208370e-01,  1.06281497e-01,\n",
      "                       1.28758654e-01,  1.16973236e-01],\n",
      "                     [ 1.82218999e-01,  1.11444063e-01, -1.62924141e-01,\n",
      "                       3.24953087e-02, -1.67053342e-01],\n",
      "                     [ 4.31294113e-01,  2.08004564e-01,  1.47714227e-01,\n",
      "                      -8.51502866e-02, -1.26487061e-01],\n",
      "                     [ 3.29497308e-01,  1.08470365e-01, -4.01340067e-01,\n",
      "                       1.66956007e-01,  5.74723601e-01],\n",
      "                     [-3.84744734e-01, -3.75315547e-01, -5.35782129e-02,\n",
      "                      -2.51350880e-01, -4.78640765e-01]], dtype=float32),\n",
      "        bias: DeviceArray([0., 0., 0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "})\n",
      "(20, 10)\n",
      "(20, 5)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "n_training_steps = 100\n",
    "\n",
    "\n",
    "[ 0.06141227  0.00766806 -0.05698649 ...  0.01272237  0.01838974\n",
    "   0.02176958]\n",
    "\n",
    "## =====================================================================\n",
    "# Random number generator sequence.\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng1, rng2 = jax.random.split(rng)\n",
    "\n",
    "# Create a one linear layer instance.\n",
    "model = nn.Dense(features=5)\n",
    "\n",
    "# Initialise the parameters.\n",
    "params = model.init(rng2, jax.random.normal(rng1, (10,)))\n",
    "\n",
    "\n",
    "print(params)\n",
    "\n",
    "## =====================================================================\n",
    "\n",
    "# Set problem dimensions.\n",
    "nsamples = 20\n",
    "xdim = 10\n",
    "ydim = 5\n",
    "\n",
    "# Generate random ground truth w and b.\n",
    "w = jax.random.normal(rng1, (xdim, ydim))\n",
    "b = jax.random.normal(rng2, (ydim,))\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "ksample, knoise = jax.random.split(rng1)\n",
    "x_samples = jax.random.normal(ksample, (nsamples, xdim))\n",
    "y_samples = jnp.dot(x_samples, w) + b\n",
    "y_samples += 0.1 * jax.random.normal(knoise, (nsamples, ydim))\n",
    "\n",
    "\n",
    "print(x_samples.shape)\n",
    "print(y_samples.shape)\n",
    "\n",
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func(x_batched, y_batched):\n",
    "  def mse(params):\n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x, y):\n",
    "      pred = model.apply(params, x)\n",
    "      return jnp.inner(y-pred, y-pred) / 2.0\n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c725d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        bias: DeviceArray([-0.87172574, -0.86985385,  0.8467341 ,  0.84649915,\n",
      "                     -0.39169806], dtype=float32),\n",
      "        kernel: DeviceArray([[ 0.94558173, -0.16723742,  0.3746021 , -1.0775216 ,\n",
      "                       0.25383565],\n",
      "                     [ 0.22913468,  0.47971997,  0.62425894,  0.72347826,\n",
      "                       0.20757319],\n",
      "                     [-0.6437151 , -0.1304319 ,  1.0882291 ,  0.37703192,\n",
      "                      -0.9357848 ],\n",
      "                     [-0.57490134, -0.71605146,  0.22252326,  0.37048048,\n",
      "                      -0.5051023 ],\n",
      "                     [-0.09404071,  0.6873026 , -0.84188306,  0.10151887,\n",
      "                      -0.69316685],\n",
      "                     [ 0.17136317, -0.22676271, -0.26385623,  0.10000193,\n",
      "                       0.6823034 ],\n",
      "                     [-0.48451614,  0.8493618 ,  0.6192771 , -0.74398625,\n",
      "                       0.70105404],\n",
      "                     [ 0.9236116 , -0.6056143 ,  0.874353  ,  0.7078386 ,\n",
      "                      -0.82353777],\n",
      "                     [ 0.7616005 , -0.42215997,  0.38795024, -0.7056058 ,\n",
      "                       0.00225874],\n",
      "                     [-1.1465185 ,  0.34172577, -0.9098685 , -0.42020857,\n",
      "                       0.31398246]], dtype=float32),\n",
      "    },\n",
      "})\n",
      "Loss[0] = 6.134939670562744\n",
      "Loss[1] = 5.963257312774658\n",
      "Loss[2] = 5.795743465423584\n",
      "Loss[3] = 5.6323113441467285\n",
      "Loss[4] = 5.472840785980225\n",
      "Loss[5] = 5.317233562469482\n",
      "Loss[6] = 5.165406227111816\n",
      "Loss[7] = 5.017330169677734\n",
      "Loss[8] = 4.872988224029541\n",
      "Loss[9] = 4.732362270355225\n",
      "Loss[10] = 4.595433235168457\n",
      "Loss[11] = 4.462172508239746\n",
      "Loss[12] = 4.332527160644531\n",
      "Loss[13] = 4.206422328948975\n",
      "Loss[14] = 4.083777904510498\n",
      "Loss[15] = 3.9645183086395264\n",
      "Loss[16] = 3.848566770553589\n",
      "Loss[17] = 3.7358360290527344\n",
      "Loss[18] = 3.626242160797119\n",
      "Loss[19] = 3.5197198390960693\n",
      "Loss[20] = 3.416219711303711\n",
      "Loss[21] = 3.3156991004943848\n",
      "Loss[22] = 3.2181050777435303\n",
      "Loss[23] = 3.1233744621276855\n",
      "Loss[24] = 3.031440496444702\n",
      "Loss[25] = 2.9422357082366943\n",
      "Loss[26] = 2.85569429397583\n",
      "Loss[27] = 2.7717514038085938\n",
      "Loss[28] = 2.690345287322998\n",
      "Loss[29] = 2.611414909362793\n",
      "Loss[30] = 2.5348973274230957\n",
      "Loss[31] = 2.460725784301758\n",
      "Loss[32] = 2.3888328075408936\n",
      "Loss[33] = 2.3191494941711426\n",
      "Loss[34] = 2.2516067028045654\n",
      "Loss[35] = 2.1861374378204346\n",
      "Loss[36] = 2.1226770877838135\n",
      "Loss[37] = 2.0611660480499268\n",
      "Loss[38] = 2.0015456676483154\n",
      "Loss[39] = 1.9437553882598877\n",
      "Loss[40] = 1.8877366781234741\n",
      "Loss[41] = 1.8334320783615112\n",
      "Loss[42] = 1.7807854413986206\n",
      "Loss[43] = 1.729744553565979\n",
      "Loss[44] = 1.6802581548690796\n",
      "Loss[45] = 1.6322784423828125\n",
      "Loss[46] = 1.5857588052749634\n",
      "Loss[47] = 1.5406550168991089\n",
      "Loss[48] = 1.4969249963760376\n",
      "Loss[49] = 1.4545286893844604\n",
      "Loss[50] = 1.413428783416748\n",
      "Loss[51] = 1.3735886812210083\n",
      "Loss[52] = 1.3349727392196655\n",
      "Loss[53] = 1.2975467443466187\n",
      "Loss[54] = 1.2612781524658203\n",
      "Loss[55] = 1.226134181022644\n",
      "Loss[56] = 1.1920844316482544\n",
      "Loss[57] = 1.1590975522994995\n",
      "Loss[58] = 1.1271440982818604\n",
      "Loss[59] = 1.0961947441101074\n",
      "Loss[60] = 1.0662205219268799\n",
      "Loss[61] = 1.037192463874817\n",
      "Loss[62] = 1.009082317352295\n",
      "Loss[63] = 0.9818618893623352\n",
      "Loss[64] = 0.955504834651947\n",
      "Loss[65] = 0.9299840927124023\n",
      "Loss[66] = 0.9052730798721313\n",
      "Loss[67] = 0.8813462257385254\n",
      "Loss[68] = 0.8581782579421997\n",
      "Loss[69] = 0.8357442021369934\n",
      "Loss[70] = 0.8140198588371277\n",
      "Loss[71] = 0.7929813265800476\n",
      "Loss[72] = 0.7726053595542908\n",
      "Loss[73] = 0.7528693675994873\n",
      "Loss[74] = 0.733751654624939\n",
      "Loss[75] = 0.7152304649353027\n",
      "Loss[76] = 0.6972848773002625\n",
      "Loss[77] = 0.6798946261405945\n",
      "Loss[78] = 0.6630403995513916\n",
      "Loss[79] = 0.6467028260231018\n",
      "Loss[80] = 0.630863606929779\n",
      "Loss[81] = 0.6155051589012146\n",
      "Loss[82] = 0.6006102561950684\n",
      "Loss[83] = 0.5861623883247375\n",
      "Loss[84] = 0.572145402431488\n",
      "Loss[85] = 0.5585439205169678\n",
      "Loss[86] = 0.5453430414199829\n",
      "Loss[87] = 0.5325283408164978\n",
      "Loss[88] = 0.5200861096382141\n",
      "Loss[89] = 0.5080030560493469\n",
      "Loss[90] = 0.4962661862373352\n",
      "Loss[91] = 0.4848637282848358\n",
      "Loss[92] = 0.47378331422805786\n",
      "Loss[93] = 0.46301397681236267\n",
      "Loss[94] = 0.4525446891784668\n",
      "Loss[95] = 0.4423651397228241\n",
      "Loss[96] = 0.432465136051178\n",
      "Loss[97] = 0.42283496260643005\n",
      "Loss[98] = 0.41346555948257446\n",
      "Loss[99] = 0.40434789657592773\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sampled loss.\n",
    "loss = make_mse_func(x_samples, y_samples)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "\n",
    "print(params)\n",
    "# Minimise the loss.\n",
    "for step in range(n_training_steps):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    print(f'Loss[{step}] = {loss_val}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(env_jax_3)",
   "language": "python",
   "name": "env_jax_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
