{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.deploy import helpers\n",
    "import graphviz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cc932",
   "metadata": {},
   "source": [
    "## Download & Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232095f7",
   "metadata": {},
   "source": [
    "## Read orm x and y data\n",
    "#### orm.parquet: processed from 01-data-preprocessing\n",
    "#### labels.parquet: created from orm > type_editor_ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6008078d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['string', 'bool', 'string', 'linkedlist', 'string', 'linkedlist',\n",
       "       'string', 'string', 'string', 'string', 'linkedlist', 'string',\n",
       "       'string', 'string', 'linkedlist', 'linkedlist', 'linkedlist',\n",
       "       'string', 'string', 'string', 'string', 'decimal', 'decimal',\n",
       "       'decimal', 'decimal', 'linkedlist', 'linkedlist', 'linkedlist',\n",
       "       'linkedlist', 'linkedlist', 'bool', 'string', 'linkedlist',\n",
       "       'linkedlist', 'string', 'linkedlist', 'linkedlist', 'string',\n",
       "       'linkedlist', 'linkedlist', 'string', 'string', 'linkedlist',\n",
       "       'string', 'string', 'linkedlist', 'string', 'linkedlist', 'string',\n",
       "       'string', 'string', 'linkedlist', 'string', 'linkedlist', 'string',\n",
       "       'linkedlist', 'linkedlist', 'string', 'linkedlist', 'string',\n",
       "       'string', 'string', 'linkedlist', 'linkedlist', 'string', 'byte',\n",
       "       'linkedlist', 'number', 'linkedlist', 'number', 'string',\n",
       "       'linkedlist', 'string', 'linkedlist', 'string', 'string', 'number',\n",
       "       'number', 'linkedlist', 'linkedlist', 'string', 'string',\n",
       "       'linkedlist'], dtype='<U10')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## orm data\n",
    "X_orm = pd.read_parquet('../data/data/processed/orm.parquet')\n",
    "\n",
    "y_orm = pd.read_parquet('../data/data/raw/labels.parquet').values.flatten()\n",
    "y_orm = np.array([x.lower() for x in y_orm])\n",
    "\n",
    "y_orm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37380ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train & validation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_orm_train, X_orm_val, y_orm_train, y_orm_val = train_test_split(X_orm, y_orm, test_size=0.33, random_state=42)\n",
    "\n",
    "X_orm_train.shape\n",
    "y_orm_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ff95873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n",
      "/mnt/d/GitCode/sherlock-project/sherlock/deploy/helpers.py:18: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  feature_cols_dict[feature_set] = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(y_orm_train))\n",
    "print(num_classes)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_orm_train)\n",
    "\n",
    "# Getting feature col names\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "\n",
    "\n",
    "X_train_char = X_orm_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_orm_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_orm_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_orm_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_orm_val[feature_cols[\"char\"]]\n",
    "X_val_word = X_orm_val[feature_cols[\"word\"]]\n",
    "X_val_par = X_orm_val[feature_cols[\"par\"]]\n",
    "X_val_rest = X_orm_val[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_orm_train)   \n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) \n",
    "\n",
    "y_val_int = encoder.transform(y_orm_val)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7afa8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_1 = jnp.array(pd.DataFrame(X_train_char).to_numpy())\n",
    "j_2 = jnp.array(pd.DataFrame(X_train_word).to_numpy())\n",
    "j_3 = jnp.array(pd.DataFrame(X_train_par).to_numpy())\n",
    "j_4 = jnp.array(pd.DataFrame(X_train_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33df66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "  \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4f00e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    training: bool = True\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # batchnormalisation - https://github.com/google/flax/issues/932\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        x = nn.relu(nn.Dense(self.features[0])(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "                \n",
    "        x = nn.relu(nn.Dense(self.features[1])(x)) \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eee3435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    num_classes: int = 6\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x1, x2, x3, x4):\n",
    "       \n",
    "        # [1] define shape        \n",
    "        y1 = SubModel([300, 300], name='char_model')(x1)      \n",
    "        y2 = SubModel([200, 200], name='word_model')(x2)\n",
    "        y3 = SubModel([400, 400], name='par_model')(x3)\n",
    "        y4 = RestModel([27], name='rest_model')(x4)\n",
    "                      \n",
    "        # [2] concat submodels    \n",
    "        x = jnp.concatenate((y1, y2, y3, y4), axis=-1)\n",
    "        \n",
    "        print(\"check mainmodel shape\")\n",
    "        print(np.shape(x))\n",
    "        \n",
    "        # batchnormalisation\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        # dense 1\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "        \n",
    "        # dense 2\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dense w/ softmax - todo: check\n",
    "        x = nn.softmax(nn.Dense(self.feature_size)(x), axis=-1)\n",
    "        \n",
    "        return nn.Dense(self.num_classes)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "590cbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(1, 927)\n"
     ]
    }
   ],
   "source": [
    "mainmodel = MainModel()\n",
    "p_main = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9ffb6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['char_model', 'word_model', 'par_model', 'rest_model', 'BatchNorm_0', 'Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p_main\n",
    "p_main['params'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9325b0",
   "metadata": {},
   "source": [
    "## Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42898abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(83, 927)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nn_outcome.pdf'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowered = jax.jit(mainmodel.apply).lower(p_main,j_1,j_2,j_3,j_4)\n",
    "comp_dot = graphviz.Source(lowered._xla_computation().as_hlo_dot_graph())\n",
    "comp_dot.render('nn_outcome', view=True).replace('\\\\', '/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15b0bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"application/pdf\"\n"
     ]
    }
   ],
   "source": [
    "#print(y_main.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf381d",
   "metadata": {},
   "source": [
    "## Training - OPTAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func(x_b_1, x_b_2, x_b_3, x_b_4, y_batched):\n",
    "  def mse(p_main):    \n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x1, x2, x3, x4, y):      \n",
    "      pred = mainmodel.apply(p_main, x1, x2, x3, x4)\n",
    "      return jnp.inner(y-pred, y-pred) / 2.0  \n",
    "    \n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_b_1, x_b_2, x_b_3, x_b_4, y_batched), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d7f41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = p_main\n",
    "\n",
    "#dst_x = jnp.concatenate((j_1, j_2, j_3, j_4), axis=-1)\n",
    "dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "loss = make_mse_func(j_1, j_2, j_3, j_4, dst_y)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22217fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] = 0.34637922048568726\n",
      "Loss[1] = 0.34623992443084717\n",
      "Loss[2] = 0.34610065817832947\n",
      "Loss[3] = 0.3459615409374237\n",
      "Loss[4] = 0.34582242369651794\n",
      "Loss[5] = 0.34568336606025696\n",
      "Loss[6] = 0.34554439783096313\n",
      "Loss[7] = 0.3454054594039917\n",
      "Loss[8] = 0.3452666103839874\n",
      "Loss[9] = 0.34512779116630554\n",
      "Loss[10] = 0.3449890613555908\n",
      "Loss[11] = 0.34485042095184326\n",
      "Loss[12] = 0.3447117805480957\n",
      "Loss[13] = 0.3445732295513153\n",
      "Loss[14] = 0.3444347381591797\n",
      "Loss[15] = 0.34429627656936646\n",
      "Loss[16] = 0.3441579043865204\n",
      "Loss[17] = 0.3440195918083191\n",
      "Loss[18] = 0.3438813388347626\n",
      "Loss[19] = 0.34374314546585083\n",
      "Loss[20] = 0.34360501170158386\n",
      "Loss[21] = 0.34346693754196167\n",
      "Loss[22] = 0.34332895278930664\n",
      "Loss[23] = 0.3431909680366516\n",
      "Loss[24] = 0.34305310249328613\n",
      "Loss[25] = 0.34291526675224304\n",
      "Loss[26] = 0.3427775204181671\n",
      "Loss[27] = 0.3426397740840912\n",
      "Loss[28] = 0.3425021171569824\n",
      "Loss[29] = 0.34236451983451843\n",
      "Loss[30] = 0.3422270119190216\n",
      "Loss[31] = 0.34208956360816956\n",
      "Loss[32] = 0.3419521152973175\n",
      "Loss[33] = 0.3418147563934326\n",
      "Loss[34] = 0.3416774868965149\n",
      "Loss[35] = 0.34154024720191956\n",
      "Loss[36] = 0.341403067111969\n",
      "Loss[37] = 0.3412659168243408\n",
      "Loss[38] = 0.3411288857460022\n",
      "Loss[39] = 0.3409918546676636\n",
      "Loss[40] = 0.3408549427986145\n",
      "Loss[41] = 0.3407180607318878\n",
      "Loss[42] = 0.3405812382698059\n",
      "Loss[43] = 0.3404444456100464\n",
      "Loss[44] = 0.34030774235725403\n",
      "Loss[45] = 0.34017109870910645\n",
      "Loss[46] = 0.34003451466560364\n",
      "Loss[47] = 0.339898020029068\n",
      "Loss[48] = 0.33976152539253235\n",
      "Loss[49] = 0.33962512016296387\n",
      "Loss[50] = 0.3394887447357178\n",
      "Loss[51] = 0.33935245871543884\n",
      "Loss[52] = 0.3392162322998047\n",
      "Loss[53] = 0.3390800356864929\n",
      "Loss[54] = 0.3389439284801483\n",
      "Loss[55] = 0.3388078510761261\n",
      "Loss[56] = 0.33867183327674866\n",
      "Loss[57] = 0.3385359048843384\n",
      "Loss[58] = 0.3384000360965729\n",
      "Loss[59] = 0.3382641673088074\n",
      "Loss[60] = 0.3381284177303314\n",
      "Loss[61] = 0.33799272775650024\n",
      "Loss[62] = 0.33785703778266907\n",
      "Loss[63] = 0.33772143721580505\n",
      "Loss[64] = 0.3375858962535858\n",
      "Loss[65] = 0.33745041489601135\n",
      "Loss[66] = 0.3373149633407593\n",
      "Loss[67] = 0.33717960119247437\n",
      "Loss[68] = 0.33704429864883423\n",
      "Loss[69] = 0.3369090259075165\n",
      "Loss[70] = 0.3367738127708435\n",
      "Loss[71] = 0.3366386890411377\n",
      "Loss[72] = 0.33650362491607666\n",
      "Loss[73] = 0.336368590593338\n",
      "Loss[74] = 0.33623361587524414\n",
      "Loss[75] = 0.33609870076179504\n",
      "Loss[76] = 0.33596381545066833\n",
      "Loss[77] = 0.3358290195465088\n",
      "Loss[78] = 0.335694283246994\n",
      "Loss[79] = 0.335559606552124\n",
      "Loss[80] = 0.3354249894618988\n",
      "Loss[81] = 0.3352903723716736\n",
      "Loss[82] = 0.3351558744907379\n",
      "Loss[83] = 0.335021436214447\n",
      "Loss[84] = 0.3348870277404785\n",
      "Loss[85] = 0.3347526788711548\n",
      "Loss[86] = 0.3346184194087982\n",
      "Loss[87] = 0.33448413014411926\n",
      "Loss[88] = 0.33434998989105225\n",
      "Loss[89] = 0.3342158794403076\n",
      "Loss[90] = 0.3340817987918854\n",
      "Loss[91] = 0.3339477777481079\n",
      "Loss[92] = 0.3338138163089752\n",
      "Loss[93] = 0.3336799144744873\n",
      "Loss[94] = 0.33354607224464417\n",
      "Loss[95] = 0.3334122598171234\n",
      "Loss[96] = 0.3332785367965698\n",
      "Loss[97] = 0.3331449031829834\n",
      "Loss[98] = 0.333011269569397\n",
      "Loss[99] = 0.3328777253627777\n",
      "Loss[100] = 0.33274421095848083\n",
      "Loss[101] = 0.3326107859611511\n",
      "Loss[102] = 0.332477331161499\n",
      "Loss[103] = 0.33234402537345886\n",
      "Loss[104] = 0.3322107195854187\n",
      "Loss[105] = 0.3320775032043457\n",
      "Loss[106] = 0.3319443166255951\n",
      "Loss[107] = 0.33181118965148926\n",
      "Loss[108] = 0.3316781222820282\n",
      "Loss[109] = 0.3315451443195343\n",
      "Loss[110] = 0.3314121663570404\n",
      "Loss[111] = 0.33127927780151367\n",
      "Loss[112] = 0.3311464488506317\n",
      "Loss[113] = 0.33101364970207214\n",
      "Loss[114] = 0.33088091015815735\n",
      "Loss[115] = 0.33074820041656494\n",
      "Loss[116] = 0.3306156098842621\n",
      "Loss[117] = 0.3304830491542816\n",
      "Loss[118] = 0.33035051822662354\n",
      "Loss[119] = 0.33021804690361023\n",
      "Loss[120] = 0.3300856649875641\n",
      "Loss[121] = 0.32995328307151794\n",
      "Loss[122] = 0.32982102036476135\n",
      "Loss[123] = 0.32968875765800476\n",
      "Loss[124] = 0.32955658435821533\n",
      "Loss[125] = 0.3294244706630707\n",
      "Loss[126] = 0.3292923867702484\n",
      "Loss[127] = 0.3291603624820709\n",
      "Loss[128] = 0.3290283679962158\n",
      "Loss[129] = 0.3288964629173279\n",
      "Loss[130] = 0.3287646174430847\n",
      "Loss[131] = 0.32863274216651917\n",
      "Loss[132] = 0.32850104570388794\n",
      "Loss[133] = 0.3283693492412567\n",
      "Loss[134] = 0.32823771238327026\n",
      "Loss[135] = 0.3281061351299286\n",
      "Loss[136] = 0.3279745578765869\n",
      "Loss[137] = 0.3278430998325348\n",
      "Loss[138] = 0.32771170139312744\n",
      "Loss[139] = 0.3275803029537201\n",
      "Loss[140] = 0.3274489939212799\n",
      "Loss[141] = 0.3273176848888397\n",
      "Loss[142] = 0.3271864950656891\n",
      "Loss[143] = 0.3270553648471832\n",
      "Loss[144] = 0.32692423462867737\n",
      "Loss[145] = 0.3267931640148163\n",
      "Loss[146] = 0.32666218280792236\n",
      "Loss[147] = 0.3265312612056732\n",
      "Loss[148] = 0.3264003396034241\n",
      "Loss[149] = 0.3262695372104645\n",
      "Loss[150] = 0.3261387348175049\n",
      "Loss[151] = 0.32600805163383484\n",
      "Loss[152] = 0.3258773386478424\n",
      "Loss[153] = 0.32574671506881714\n",
      "Loss[154] = 0.32561615109443665\n",
      "Loss[155] = 0.32548561692237854\n",
      "Loss[156] = 0.3253551423549652\n",
      "Loss[157] = 0.32522475719451904\n",
      "Loss[158] = 0.3250943720340729\n",
      "Loss[159] = 0.32496410608291626\n",
      "Loss[160] = 0.32483386993408203\n",
      "Loss[161] = 0.3247036635875702\n",
      "Loss[162] = 0.3245735168457031\n",
      "Loss[163] = 0.32444342970848083\n",
      "Loss[164] = 0.3243134021759033\n",
      "Loss[165] = 0.3241834044456482\n",
      "Loss[166] = 0.32405346632003784\n",
      "Loss[167] = 0.32392361760139465\n",
      "Loss[168] = 0.3237937390804291\n",
      "Loss[169] = 0.32366397976875305\n",
      "Loss[170] = 0.3235342502593994\n",
      "Loss[171] = 0.32340458035469055\n",
      "Loss[172] = 0.32327499985694885\n",
      "Loss[173] = 0.32314541935920715\n",
      "Loss[174] = 0.32301589846611023\n",
      "Loss[175] = 0.3228864371776581\n",
      "Loss[176] = 0.3227570056915283\n",
      "Loss[177] = 0.3226277232170105\n",
      "Loss[178] = 0.3224983811378479\n",
      "Loss[179] = 0.32236912846565247\n",
      "Loss[180] = 0.3222399652004242\n",
      "Loss[181] = 0.3221108019351959\n",
      "Loss[182] = 0.3219817280769348\n",
      "Loss[183] = 0.3218526542186737\n",
      "Loss[184] = 0.32172369956970215\n",
      "Loss[185] = 0.321594774723053\n",
      "Loss[186] = 0.3214658498764038\n",
      "Loss[187] = 0.3213370442390442\n",
      "Loss[188] = 0.32120826840400696\n",
      "Loss[189] = 0.3210795521736145\n",
      "Loss[190] = 0.32095083594322205\n",
      "Loss[191] = 0.32082223892211914\n",
      "Loss[192] = 0.3206936717033386\n",
      "Loss[193] = 0.3205651342868805\n",
      "Loss[194] = 0.3204366862773895\n",
      "Loss[195] = 0.32030826807022095\n",
      "Loss[196] = 0.32017987966537476\n",
      "Loss[197] = 0.3200515806674957\n",
      "Loss[198] = 0.3199233114719391\n",
      "Loss[199] = 0.3197951316833496\n",
      "Loss[200] = 0.3196669816970825\n",
      "Loss[201] = 0.31953883171081543\n",
      "Loss[202] = 0.3194108307361603\n",
      "Loss[203] = 0.31928279995918274\n",
      "Loss[204] = 0.31915485858917236\n",
      "Loss[205] = 0.3190269470214844\n",
      "Loss[206] = 0.31889912486076355\n",
      "Loss[207] = 0.3187713325023651\n",
      "Loss[208] = 0.31864356994628906\n",
      "Loss[209] = 0.3185158669948578\n",
      "Loss[210] = 0.3183882236480713\n",
      "Loss[211] = 0.31826066970825195\n",
      "Loss[212] = 0.3181331157684326\n",
      "Loss[213] = 0.31800562143325806\n",
      "Loss[214] = 0.3178781569004059\n",
      "Loss[215] = 0.31775081157684326\n",
      "Loss[216] = 0.31762343645095825\n",
      "Loss[217] = 0.3174961805343628\n",
      "Loss[218] = 0.31736892461776733\n",
      "Loss[219] = 0.31724175810813904\n",
      "Loss[220] = 0.31711462140083313\n",
      "Loss[221] = 0.316987544298172\n",
      "Loss[222] = 0.31686049699783325\n",
      "Loss[223] = 0.31673353910446167\n",
      "Loss[224] = 0.3166066110134125\n",
      "Loss[225] = 0.31647974252700806\n",
      "Loss[226] = 0.3163529336452484\n",
      "Loss[227] = 0.31622612476348877\n",
      "Loss[228] = 0.3160994350910187\n",
      "Loss[229] = 0.31597277522087097\n",
      "Loss[230] = 0.31584611535072327\n",
      "Loss[231] = 0.3157195448875427\n",
      "Loss[232] = 0.31559300422668457\n",
      "Loss[233] = 0.31546658277511597\n",
      "Loss[234] = 0.315340131521225\n",
      "Loss[235] = 0.31521373987197876\n",
      "Loss[236] = 0.3150874674320221\n",
      "Loss[237] = 0.31496119499206543\n",
      "Loss[238] = 0.31483498215675354\n",
      "Loss[239] = 0.31470879912376404\n",
      "Loss[240] = 0.3145826756954193\n",
      "Loss[241] = 0.31445664167404175\n",
      "Loss[242] = 0.3143306076526642\n",
      "Loss[243] = 0.3142046630382538\n",
      "Loss[244] = 0.3140787184238434\n",
      "Loss[245] = 0.31395289301872253\n",
      "Loss[246] = 0.3138270378112793\n",
      "Loss[247] = 0.3137013018131256\n",
      "Loss[248] = 0.3135755658149719\n",
      "Loss[249] = 0.3134499192237854\n",
      "Loss[250] = 0.3133242726325989\n",
      "Loss[251] = 0.3131987452507019\n",
      "Loss[252] = 0.3130732476711273\n",
      "Loss[253] = 0.3129478096961975\n",
      "Loss[254] = 0.3128223419189453\n",
      "Loss[255] = 0.31269702315330505\n",
      "Loss[256] = 0.3125716745853424\n",
      "Loss[257] = 0.3124464452266693\n",
      "Loss[258] = 0.3123212158679962\n",
      "Loss[259] = 0.3121960461139679\n",
      "Loss[260] = 0.31207093596458435\n",
      "Loss[261] = 0.3119458854198456\n",
      "Loss[262] = 0.3118208944797516\n",
      "Loss[263] = 0.3116959035396576\n",
      "Loss[264] = 0.31157100200653076\n",
      "Loss[265] = 0.3114461302757263\n",
      "Loss[266] = 0.31132131814956665\n",
      "Loss[267] = 0.31119653582572937\n",
      "Loss[268] = 0.31107181310653687\n",
      "Loss[269] = 0.3109471797943115\n",
      "Loss[270] = 0.3108225464820862\n",
      "Loss[271] = 0.3106979727745056\n",
      "Loss[272] = 0.3105734884738922\n",
      "Loss[273] = 0.3104490041732788\n",
      "Loss[274] = 0.3103245794773102\n",
      "Loss[275] = 0.31020021438598633\n",
      "Loss[276] = 0.31007590889930725\n",
      "Loss[277] = 0.30995163321495056\n",
      "Loss[278] = 0.30982741713523865\n",
      "Loss[279] = 0.3097032606601715\n",
      "Loss[280] = 0.30957913398742676\n",
      "Loss[281] = 0.3094550669193268\n",
      "Loss[282] = 0.3093310296535492\n",
      "Loss[283] = 0.3092070519924164\n",
      "Loss[284] = 0.30908313393592834\n",
      "Loss[285] = 0.3089592456817627\n",
      "Loss[286] = 0.3088354170322418\n",
      "Loss[287] = 0.3087116777896881\n",
      "Loss[288] = 0.3085879385471344\n",
      "Loss[289] = 0.3084642291069031\n",
      "Loss[290] = 0.3083406388759613\n",
      "Loss[291] = 0.30821698904037476\n",
      "Loss[292] = 0.30809351801872253\n",
      "Loss[293] = 0.3079700171947479\n",
      "Loss[294] = 0.3078466057777405\n",
      "Loss[295] = 0.30772319436073303\n",
      "Loss[296] = 0.30759987235069275\n",
      "Loss[297] = 0.30747658014297485\n",
      "Loss[298] = 0.3073533773422241\n",
      "Loss[299] = 0.3072301745414734\n",
      "Loss[300] = 0.30710703134536743\n",
      "Loss[301] = 0.30698394775390625\n",
      "Loss[302] = 0.30686089396476746\n",
      "Loss[303] = 0.30673789978027344\n",
      "Loss[304] = 0.3066149652004242\n",
      "Loss[305] = 0.30649203062057495\n",
      "Loss[306] = 0.30636921525001526\n",
      "Loss[307] = 0.30624639987945557\n",
      "Loss[308] = 0.30612364411354065\n",
      "Loss[309] = 0.3060009479522705\n",
      "Loss[310] = 0.30587831139564514\n",
      "Loss[311] = 0.30575570464134216\n",
      "Loss[312] = 0.30563315749168396\n",
      "Loss[313] = 0.30551064014434814\n",
      "Loss[314] = 0.3053881525993347\n",
      "Loss[315] = 0.30526578426361084\n",
      "Loss[316] = 0.30514341592788696\n",
      "Loss[317] = 0.3050210773944855\n",
      "Loss[318] = 0.30489882826805115\n",
      "Loss[319] = 0.3047766089439392\n",
      "Loss[320] = 0.30465441942214966\n",
      "Loss[321] = 0.30453231930732727\n",
      "Loss[322] = 0.3044102191925049\n",
      "Loss[323] = 0.30428820848464966\n",
      "Loss[324] = 0.3041662275791168\n",
      "Loss[325] = 0.30404433608055115\n",
      "Loss[326] = 0.3039224445819855\n",
      "Loss[327] = 0.3038005828857422\n",
      "Loss[328] = 0.30367881059646606\n",
      "Loss[329] = 0.3035570979118347\n",
      "Loss[330] = 0.30343538522720337\n",
      "Loss[331] = 0.3033137321472168\n",
      "Loss[332] = 0.303192138671875\n",
      "Loss[333] = 0.30307063460350037\n",
      "Loss[334] = 0.30294913053512573\n",
      "Loss[335] = 0.3028276562690735\n",
      "Loss[336] = 0.3027062714099884\n",
      "Loss[337] = 0.3025849461555481\n",
      "Loss[338] = 0.3024636209011078\n",
      "Loss[339] = 0.30234235525131226\n",
      "Loss[340] = 0.3022211790084839\n",
      "Loss[341] = 0.3021000325679779\n",
      "Loss[342] = 0.3019788861274719\n",
      "Loss[343] = 0.3018577992916107\n",
      "Loss[344] = 0.3017368018627167\n",
      "Loss[345] = 0.3016158640384674\n",
      "Loss[346] = 0.30149489641189575\n",
      "Loss[347] = 0.30137404799461365\n",
      "Loss[348] = 0.30125319957733154\n",
      "Loss[349] = 0.3011324405670166\n",
      "Loss[350] = 0.30101174116134644\n",
      "Loss[351] = 0.30089104175567627\n",
      "Loss[352] = 0.3007704019546509\n",
      "Loss[353] = 0.3006497919559479\n",
      "Loss[354] = 0.30052927136421204\n",
      "Loss[355] = 0.3004087805747986\n",
      "Loss[356] = 0.3002883195877075\n",
      "Loss[357] = 0.3001679480075836\n",
      "Loss[358] = 0.3000475764274597\n",
      "Loss[359] = 0.299927294254303\n",
      "Loss[360] = 0.29980701208114624\n",
      "Loss[361] = 0.29968684911727905\n",
      "Loss[362] = 0.29956668615341187\n",
      "Loss[363] = 0.2994465231895447\n",
      "Loss[364] = 0.29932647943496704\n",
      "Loss[365] = 0.2992064654827118\n",
      "Loss[366] = 0.29908648133277893\n",
      "Loss[367] = 0.29896655678749084\n",
      "Loss[368] = 0.29884669184684753\n",
      "Loss[369] = 0.2987268567085266\n",
      "Loss[370] = 0.29860708117485046\n",
      "Loss[371] = 0.2984873354434967\n",
      "Loss[372] = 0.2983676493167877\n",
      "Loss[373] = 0.2982479929924011\n",
      "Loss[374] = 0.2981283962726593\n",
      "Loss[375] = 0.29800885915756226\n",
      "Loss[376] = 0.29788938164711\n",
      "Loss[377] = 0.2977699041366577\n",
      "Loss[378] = 0.2976504862308502\n",
      "Loss[379] = 0.2975311279296875\n",
      "Loss[380] = 0.29741185903549194\n",
      "Loss[381] = 0.297292560338974\n",
      "Loss[382] = 0.2971733510494232\n",
      "Loss[383] = 0.2970542013645172\n",
      "Loss[384] = 0.2969350218772888\n",
      "Loss[385] = 0.29681596159935\n",
      "Loss[386] = 0.2966969311237335\n",
      "Loss[387] = 0.29657796025276184\n",
      "Loss[388] = 0.29645901918411255\n",
      "Loss[389] = 0.29634013772010803\n",
      "Loss[390] = 0.2962212860584259\n",
      "Loss[391] = 0.29610249400138855\n",
      "Loss[392] = 0.2959837317466736\n",
      "Loss[393] = 0.2958650588989258\n",
      "Loss[394] = 0.295746386051178\n",
      "Loss[395] = 0.29562777280807495\n",
      "Loss[396] = 0.2955092191696167\n",
      "Loss[397] = 0.29539069533348083\n",
      "Loss[398] = 0.29527223110198975\n",
      "Loss[399] = 0.29515379667282104\n",
      "Loss[400] = 0.2950354218482971\n",
      "Loss[401] = 0.2949170768260956\n",
      "Loss[402] = 0.2947987914085388\n",
      "Loss[403] = 0.29468056559562683\n",
      "Loss[404] = 0.2945623993873596\n",
      "Loss[405] = 0.2944442331790924\n",
      "Loss[406] = 0.29432612657546997\n",
      "Loss[407] = 0.2942081093788147\n",
      "Loss[408] = 0.2940900921821594\n",
      "Loss[409] = 0.2939721345901489\n",
      "Loss[410] = 0.2938542366027832\n",
      "Loss[411] = 0.29373636841773987\n",
      "Loss[412] = 0.2936185598373413\n",
      "Loss[413] = 0.29350078105926514\n",
      "Loss[414] = 0.29338303208351135\n",
      "Loss[415] = 0.29326537251472473\n",
      "Loss[416] = 0.2931477129459381\n",
      "Loss[417] = 0.29303014278411865\n",
      "Loss[418] = 0.2929125726222992\n",
      "Loss[419] = 0.2927950918674469\n",
      "Loss[420] = 0.292677640914917\n",
      "Loss[421] = 0.29256024956703186\n",
      "Loss[422] = 0.2924429178237915\n",
      "Loss[423] = 0.29232558608055115\n",
      "Loss[424] = 0.29220831394195557\n",
      "Loss[425] = 0.29209110140800476\n",
      "Loss[426] = 0.29197391867637634\n",
      "Loss[427] = 0.2918568253517151\n",
      "Loss[428] = 0.29173973202705383\n",
      "Loss[429] = 0.29162266850471497\n",
      "Loss[430] = 0.29150569438934326\n",
      "Loss[431] = 0.29138875007629395\n",
      "Loss[432] = 0.2912718653678894\n",
      "Loss[433] = 0.29115501046180725\n",
      "Loss[434] = 0.2910382151603699\n",
      "Loss[435] = 0.2909214496612549\n",
      "Loss[436] = 0.29080474376678467\n",
      "Loss[437] = 0.29068806767463684\n",
      "Loss[438] = 0.2905714511871338\n",
      "Loss[439] = 0.2904548645019531\n",
      "Loss[440] = 0.29033833742141724\n",
      "Loss[441] = 0.2902218699455261\n",
      "Loss[442] = 0.2901054322719574\n",
      "Loss[443] = 0.28998905420303345\n",
      "Loss[444] = 0.2898726761341095\n",
      "Loss[445] = 0.2897563874721527\n",
      "Loss[446] = 0.2896401286125183\n",
      "Loss[447] = 0.2895239293575287\n",
      "Loss[448] = 0.28940773010253906\n",
      "Loss[449] = 0.289291650056839\n",
      "Loss[450] = 0.28917554020881653\n",
      "Loss[451] = 0.28905951976776123\n",
      "Loss[452] = 0.2889435589313507\n",
      "Loss[453] = 0.2888276278972626\n",
      "Loss[454] = 0.2887117266654968\n",
      "Loss[455] = 0.28859591484069824\n",
      "Loss[456] = 0.28848007321357727\n",
      "Loss[457] = 0.28836432099342346\n",
      "Loss[458] = 0.28824862837791443\n",
      "Loss[459] = 0.2881329655647278\n",
      "Loss[460] = 0.2880173623561859\n",
      "Loss[461] = 0.28790178894996643\n",
      "Loss[462] = 0.2877862751483917\n",
      "Loss[463] = 0.287670761346817\n",
      "Loss[464] = 0.2875553369522095\n",
      "Loss[465] = 0.2874399423599243\n",
      "Loss[466] = 0.28732460737228394\n",
      "Loss[467] = 0.28720927238464355\n",
      "Loss[468] = 0.28709402680397034\n",
      "Loss[469] = 0.2869788408279419\n",
      "Loss[470] = 0.28686365485191345\n",
      "Loss[471] = 0.2867485582828522\n",
      "Loss[472] = 0.2866334915161133\n",
      "Loss[473] = 0.28651848435401917\n",
      "Loss[474] = 0.28640347719192505\n",
      "Loss[475] = 0.2862885594367981\n",
      "Loss[476] = 0.28617364168167114\n",
      "Loss[477] = 0.28605881333351135\n",
      "Loss[478] = 0.28594401478767395\n",
      "Loss[479] = 0.28582924604415894\n",
      "Loss[480] = 0.2857145667076111\n",
      "Loss[481] = 0.28559988737106323\n",
      "Loss[482] = 0.28548523783683777\n",
      "Loss[483] = 0.28537067770957947\n",
      "Loss[484] = 0.28525614738464355\n",
      "Loss[485] = 0.28514164686203003\n",
      "Loss[486] = 0.28502723574638367\n",
      "Loss[487] = 0.2849128246307373\n",
      "Loss[488] = 0.2847984731197357\n",
      "Loss[489] = 0.2846841812133789\n",
      "Loss[490] = 0.2845698893070221\n",
      "Loss[491] = 0.28445565700531006\n",
      "Loss[492] = 0.2843415141105652\n",
      "Loss[493] = 0.2842273712158203\n",
      "Loss[494] = 0.2841132879257202\n",
      "Loss[495] = 0.2839992642402649\n",
      "Loss[496] = 0.28388527035713196\n",
      "Loss[497] = 0.2837713062763214\n",
      "Loss[498] = 0.28365740180015564\n",
      "Loss[499] = 0.28354358673095703\n",
      "process took 0:00:43.278441 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "start = datetime.now()\n",
    "\n",
    "for step in range(500):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "     \n",
    "    print(f'Loss[{step}] = {loss_val}')\n",
    "        \n",
    "print(f'process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1601d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aafc0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_v_1 = jnp.array(pd.DataFrame(X_val_char).to_numpy())\n",
    "j_v_2 = jnp.array(pd.DataFrame(X_val_word).to_numpy())\n",
    "j_v_3 = jnp.array(pd.DataFrame(X_val_par).to_numpy())\n",
    "j_v_4 = jnp.array(pd.DataFrame(X_val_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check mainmodel shape\n",
      "(28, 927)\n",
      "(28, 6)\n"
     ]
    }
   ],
   "source": [
    "# print(dst_x.shape) #(83, 927)\n",
    "y_pred = mainmodel.apply(params, j_v_1, j_v_2, j_v_3, j_v_4)\n",
    "print(y_pred.shape) #(83, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e4c6e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 4.87804562e-02,  2.46718861e-02,  1.02863088e-02,\n",
       "               3.64181288e-02,  4.84408438e-03,  2.75146246e-01],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [ 4.41260636e-06, -4.24693152e-03,  2.42129385e-01,\n",
       "               7.26549476e-02, -2.93187797e-04,  2.30263993e-02],\n",
       "             [ 4.87804674e-02,  2.46718936e-02,  1.02862492e-02,\n",
       "               3.64181250e-02,  4.84408438e-03,  2.75146335e-01],\n",
       "             [ 4.87804636e-02,  2.46718861e-02,  1.02862418e-02,\n",
       "               3.64181437e-02,  4.84407693e-03,  2.75146306e-01],\n",
       "             [-4.10713255e-07,  3.00034881e-05,  1.27497092e-02,\n",
       "               2.80934334e-01,  1.01101041e-01,  5.54064922e-02],\n",
       "             [-4.16301191e-07,  3.00016254e-05,  1.27497092e-02,\n",
       "               2.80934364e-01,  1.01101056e-01,  5.54064699e-02],\n",
       "             [-4.16301191e-07,  3.00016254e-05,  1.27497092e-02,\n",
       "               2.80934364e-01,  1.01101056e-01,  5.54064699e-02],\n",
       "             [ 4.87803929e-02,  2.46718563e-02,  1.02863908e-02,\n",
       "               3.64183709e-02,  4.84417006e-03,  2.75145978e-01],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [ 4.87804674e-02,  2.46718936e-02,  1.02862567e-02,\n",
       "               3.64181213e-02,  4.84408438e-03,  2.75146306e-01],\n",
       "             [-4.14438546e-07,  2.99997628e-05,  1.27497241e-02,\n",
       "               2.80934334e-01,  1.01101041e-01,  5.54064736e-02],\n",
       "             [-4.11644578e-07,  3.00016254e-05,  1.27497166e-02,\n",
       "               2.80934334e-01,  1.01101041e-01,  5.54064848e-02],\n",
       "             [-4.04193997e-07,  2.99941748e-05,  1.27498060e-02,\n",
       "               2.80934244e-01,  1.01100981e-01,  5.54064922e-02],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [-4.06987965e-07,  3.00034881e-05,  1.27497166e-02,\n",
       "               2.80934304e-01,  1.01101026e-01,  5.54065108e-02],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [-4.16301191e-07,  2.99997628e-05,  1.27497092e-02,\n",
       "               2.80934364e-01,  1.01101056e-01,  5.54064699e-02],\n",
       "             [-4.07919288e-07,  3.00034881e-05,  1.27497204e-02,\n",
       "               2.80934334e-01,  1.01101026e-01,  5.54064997e-02],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [-1.94646418e-07,  3.00817192e-05,  1.27498358e-02,\n",
       "               2.80933201e-01,  1.01100489e-01,  5.54074422e-02],\n",
       "             [-4.08850610e-07,  3.00016254e-05,  1.27497166e-02,\n",
       "               2.80934334e-01,  1.01101026e-01,  5.54065034e-02],\n",
       "             [ 4.87804674e-02,  2.46718936e-02,  1.02862418e-02,\n",
       "               3.64181250e-02,  4.84408438e-03,  2.75146335e-01],\n",
       "             [ 4.87804711e-02,  2.46718973e-02,  1.02862269e-02,\n",
       "               3.64181176e-02,  4.84408438e-03,  2.75146365e-01],\n",
       "             [-4.05125320e-07,  3.00072134e-05,  1.27497166e-02,\n",
       "               2.80934304e-01,  1.01101026e-01,  5.54065146e-02],\n",
       "             [-4.14438546e-07,  2.99997628e-05,  1.27497129e-02,\n",
       "               2.80934334e-01,  1.01101041e-01,  5.54064773e-02]],            dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d78297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _get_categorical_label_encodings(y_train, y_val, model_id: str) -> (list, list):\n",
    "\n",
    "    # Prepare categorical label encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "\n",
    "    np.save(f\"../model_files/classes_{model_id}.npy\", encoder.classes_)\n",
    "\n",
    "    # Convert train labels\n",
    "    y_train_int = encoder.transform(y_train)                 #(,83)\n",
    "    y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(83,6)\n",
    "    print(y_train_cat.shape)\n",
    "\n",
    "    \n",
    "    # Convert val labels\n",
    "    y_val_int = encoder.transform(y_val)\n",
    "    y_val_cat = tf.keras.utils.to_categorical(y_val_int)\n",
    "\n",
    "    return y_train_cat, y_val_cat\n",
    "\n",
    "_ = _get_categorical_label_encodings(y_orm_train, y_orm_val, \"orm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb8eb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes ['bool' 'byte' 'decimal' 'linkedlist' 'number' 'string']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['string', 'string', 'decimal', 'string', 'string', 'linkedlist',\n",
       "       'linkedlist', 'linkedlist', 'string', 'string', 'string',\n",
       "       'linkedlist', 'linkedlist', 'linkedlist', 'string', 'linkedlist',\n",
       "       'string', 'linkedlist', 'linkedlist', 'string', 'string', 'string',\n",
       "       'linkedlist', 'linkedlist', 'string', 'string', 'linkedlist',\n",
       "       'linkedlist'], dtype='<U10')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _proba_to_classes(y_pred, model_id: str = \"sherlock\") -> np.array:\n",
    "    y_pred_int = np.argmax(y_pred, axis=1)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(\n",
    "        f\"../model_files/classes_{model_id}.npy\", allow_pickle=True\n",
    "    )\n",
    "\n",
    "    print(\"classes\",encoder.classes_)\n",
    "\n",
    "    y_pred = encoder.inverse_transform(y_pred_int)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "_proba_to_classes(y_pred, \"orm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac1ea0",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a7197ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes ['bool' 'byte' 'decimal' 'linkedlist' 'number' 'string']\n",
      "['string' 'string' 'decimal' 'string' 'string' 'linkedlist' 'linkedlist'\n",
      " 'linkedlist' 'string' 'string' 'string' 'linkedlist' 'linkedlist'\n",
      " 'linkedlist' 'string' 'linkedlist' 'string' 'linkedlist' 'linkedlist'\n",
      " 'string' 'string' 'string' 'linkedlist' 'linkedlist' 'string' 'string'\n",
      " 'linkedlist' 'linkedlist']\n",
      "(28,)\n",
      "0.8429232804232804\n",
      "0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = _proba_to_classes(y_pred, \"orm\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_orm_val, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_orm_val, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4effbd762d6eb796ccf5bf9e233d3c9151be40d486c574f3d7f50d75226bfe66"
  },
  "kernelspec": {
   "display_name": "Python(env_jax_3)",
   "language": "python",
   "name": "env_jax_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
