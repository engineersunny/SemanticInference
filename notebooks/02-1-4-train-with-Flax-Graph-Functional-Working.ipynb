{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site-packages/distutils-precedence.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/sunnykim/miniconda3/envs/sherlock/lib/python3.7/site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\n",
      "\n",
      "Remainder of file ignored\n",
      "2023-03-21 10:48:58.881656: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 10:48:58.881769: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 10:48:58.881782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "from sherlock.deploy.model import SherlockModel\n",
    "from sherlock.deploy import helpers\n",
    "import graphviz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c188a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/google/jax/discussions/10323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4fa539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0),\n",
       " StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9577d",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09c4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data process took 0:00:02.232667 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_parquet('../data/data/processed/train.parquet')\n",
    "y_train = pd.read_parquet('../data/data/raw/train_labels.parquet').values.flatten()\n",
    "y_train = np.array([x.lower() for x in y_train])\n",
    "\n",
    "X_validation = pd.read_parquet('../data/data/processed/validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/data/raw/val_labels.parquet').values.flatten()\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "X_test = pd.read_parquet('../data/data/processed/test.parquet')\n",
    "y_test = pd.read_parquet('../data/data/raw/test_labels.parquet').values.flatten()\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print(f'Load data process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145ddbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412059, 1588)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff95873",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(y_train))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "\n",
    "feature_cols = helpers.categorize_features()\n",
    "\n",
    "X_train_char = X_train[feature_cols[\"char\"]]\n",
    "X_train_word = X_train[feature_cols[\"word\"]]\n",
    "X_train_par = X_train[feature_cols[\"par\"]]\n",
    "X_train_rest = X_train[feature_cols[\"rest\"]]\n",
    "\n",
    "X_val_char = X_validation[feature_cols[\"char\"]]\n",
    "X_val_word = X_validation[feature_cols[\"word\"]]\n",
    "X_val_par = X_validation[feature_cols[\"par\"]]\n",
    "X_val_rest = X_validation[feature_cols[\"rest\"]]\n",
    "\n",
    "y_train_int = encoder.transform(y_train)   #(412059,)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_int) #(412059,78)\n",
    "\n",
    "y_val_int = encoder.transform(y_validation)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val_int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16acbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(pd.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a10505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train     # labels #(412059,)\n",
    "y_train_int # numeric labels #(412059,)\n",
    "y_train_cat # (412059,78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04eebd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loads data onto gpu\n",
    "# j_1 = jnp.array(pd.DataFrame(X_train_char).to_numpy())\n",
    "# j_2 = jnp.array(pd.DataFrame(X_train_word).to_numpy())\n",
    "# j_3 = jnp.array(pd.DataFrame(X_train_par).to_numpy())\n",
    "# j_4 = jnp.array(pd.DataFrame(X_train_rest).to_numpy())\n",
    "\n",
    "#j_2.device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a7f8",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09988abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "  \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04987e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubModel(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    training: bool = True\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # batchnormalisation - https://github.com/google/flax/issues/932\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        x = nn.relu(nn.Dense(self.features[0])(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "                \n",
    "        x = nn.relu(nn.Dense(self.features[1])(x)) \n",
    "        \n",
    "        # todo: add regulariser\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128e876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[4146024105  967050713]\n",
      "[0.14389051]\n",
      "[-2.6105583   0.03385283  1.0863333  -1.480299    0.48895672  1.062516\n",
      "  0.54174834  0.0170228   0.2722685   0.30522448]\n"
     ]
    }
   ],
   "source": [
    "# PRNGKey Example\n",
    "print(random.PRNGKey(0))\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "print(key1)\n",
    "print(random.normal(key1,shape=(1,)))\n",
    "\n",
    "a = random.normal(key1, (10,))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee3435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    feature_size: int = 500\n",
    "    num_classes: int = 78\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x1, x2, x3, x4):\n",
    "       \n",
    "        # [1] define shape        \n",
    "        y1 = SubModel([300, 300], name='char_model')(x1)      \n",
    "        y2 = SubModel([200, 200], name='word_model')(x2)\n",
    "        y3 = SubModel([400, 400], name='par_model')(x3)\n",
    "        y4 = RestModel([27], name='rest_model')(x4)\n",
    "                      \n",
    "        # [2] concat submodels    \n",
    "        x = jnp.concatenate((y1, y2, y3, y4), axis=-1)\n",
    "        \n",
    "        \n",
    "        # batchnormalisation\n",
    "        x = nn.BatchNorm(use_running_average=True,\n",
    "                 momentum=0.9,\n",
    "                 epsilon=1e-5,\n",
    "                 dtype=jnp.float32)(x)\n",
    "        \n",
    "        # dense 1\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dropout\n",
    "        x = nn.Dropout(rate=0.35)(x, deterministic=True)\n",
    "        \n",
    "        # dense 2\n",
    "        x = nn.relu(nn.Dense(self.feature_size)(x))\n",
    "        \n",
    "        # dense w/ softmax - todo: check\n",
    "        x = nn.softmax(nn.Dense(self.feature_size)(x), axis=-1)\n",
    "        \n",
    "        return nn.Dense(self.num_classes)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55e709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainmodel = MainModel()\n",
    "\n",
    "# param init\n",
    "p_main = mainmodel.init(jax.random.PRNGKey(0), jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27))) \n",
    "# apply\n",
    "#y_main = mainmodel.apply(p_main,jnp.ones((1, 960)), jnp.ones((1, 201)), jnp.ones((1, 400)), jnp.ones((1, 27)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3687290f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['char_model', 'word_model', 'par_model', 'rest_model', 'BatchNorm_0', 'Dense_0', 'Dense_1', 'Dense_2', 'Dense_3'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p_main\n",
    "p_main['params'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf381d",
   "metadata": {},
   "source": [
    "## Training - OPTAX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42371a4b",
   "metadata": {},
   "source": [
    "Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f09d4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "PRNGKey = Any\n",
    "Dataset = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "875ad4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = {\n",
    "    'char': pd.DataFrame(X_train_char).to_numpy(),\n",
    "    'word': pd.DataFrame(X_train_word).to_numpy(),\n",
    "    'par': pd.DataFrame(X_train_par).to_numpy(),\n",
    "    'test': pd.DataFrame(X_train_rest).to_numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562a298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_collator(rng: PRNGKey, \n",
    "                        char_ds, word_ds, par_ds, rest_ds,\n",
    "                        labels, \n",
    "                        batch_size: int):\n",
    "    len_dataset = len(char_ds)\n",
    "    steps_per_epoch = len_dataset // batch_size\n",
    "    perms = jax.random.permutation(rng, len_dataset)\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size)) \n",
    "    \n",
    "    for perm in perms:\n",
    "        batch = {\n",
    "            'char': char_ds[perm],\n",
    "            'word': word_ds[perm],\n",
    "            'par': par_ds[perm],\n",
    "            'rest': rest_ds[perm],\n",
    "            'labels': labels[perm]\n",
    "        }\n",
    "        \n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53155817",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(123)\n",
    "rng, sample_rng = jax.random.split(rng)\n",
    "\n",
    "train_data_loader = train_data_collator(\n",
    "    sample_rng,\n",
    "    pd.DataFrame(X_train_char).to_numpy(),\n",
    "    pd.DataFrame(X_train_word).to_numpy(),\n",
    "    pd.DataFrame(X_train_par).to_numpy(),\n",
    "    pd.DataFrame(X_train_rest).to_numpy(),\n",
    "    onehot(y_train_int, num_labels),#dst_y,\n",
    "    128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d6db273",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dceb2810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 960)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['char'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f58d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "n_training_steps = 100\n",
    "\n",
    "# # Define an MSE loss function.\n",
    "# def make_mse_func(x_b_1, x_b_2, x_b_3, x_b_4, y_batched):\n",
    "#   def mse(p_main):    \n",
    "#     # Define the squared loss for a single (x, y) pair.\n",
    "#     def squared_error(x1, x2, x3, x4, y):      \n",
    "#       pred = mainmodel.apply(p_main, x1, x2, x3, x4)\n",
    "#       return jnp.inner(y-pred, y-pred) / 2.0  \n",
    "    \n",
    "#     # Vectorise the squared error and compute the average of the loss.\n",
    "#     return jnp.mean(jax.vmap(squared_error)(x_b_1, x_b_2, x_b_3, x_b_4, y_batched), axis=0)\n",
    "#   return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc89932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an MSE loss function.\n",
    "def make_mse_func():\n",
    "  def mse(p_main, batch):    \n",
    "    # Define the squared loss for a single (x, y) pair.\n",
    "    def squared_error(x1, x2, x3, x4, y):      \n",
    "      pred = mainmodel.apply(p_main, x1, x2, x3, x4)\n",
    "      return jnp.inner(y-pred, y-pred) / 2.0  \n",
    "    \n",
    "    # Vectorise the squared error and compute the average of the loss.\n",
    "    return jnp.mean(jax.vmap(squared_error)(\n",
    "        batch['char'],\n",
    "        batch['word'],\n",
    "        batch['par'],\n",
    "        batch['rest'],\n",
    "        batch['labels']), axis=0)\n",
    "  return jax.jit(mse)  # `jit` the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1098a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train     # labels #(412059,)\n",
    "y_train_int # numeric labels #(412059,)\n",
    "y_train_cat # (412059,78)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d7f41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = p_main\n",
    "\n",
    "#dst_x = jnp.concatenate((j_1, j_2, j_3, j_4), axis=-1)\n",
    "dst_y = jnp.array(y_train_cat)\n",
    "\n",
    "# Instantiate the sampled loss.\n",
    "# loss = make_mse_func(j_1[:128], j_2[:128], j_3[:128], j_4[:128], dst_y[:128])\n",
    "\n",
    "loss = make_mse_func()\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Compute the gradient of the loss function.\n",
    "loss_grad_fn = jax.value_and_grad(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66d9fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(params, opt_state, batch):\n",
    "    # Compute gradient of the loss.\n",
    "    loss_val, grads = loss_grad_fn(params, batch)\n",
    "    # Update the optimiser state, create an update to the params.\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    # Update the parameters.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_val\n",
    "\n",
    "train_step_fn = jax.jit(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93bcf92",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22217fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 10:49:23.656729: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.3 = f32[2048,300]{1,0} custom-call(f32[2048,960]{1,0} %add.155, f32[960,300]{1,0} %get-tuple-element.25, f32[2048,300]{1,0} %broadcast.244), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/char_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.700553: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.7 = f32[2048,300]{1,0} custom-call(f32[2048,300]{1,0} %maximum.33, f32[300,300]{1,0} %get-tuple-element.27, f32[2048,300]{1,0} %broadcast.248), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/char_model/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.728761: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.11 = f32[2048,200]{1,0} custom-call(f32[2048,201]{1,0} %add.215, f32[201,200]{1,0} %get-tuple-element.39, f32[2048,200]{1,0} %broadcast.258), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/word_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.758971: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.15 = f32[2048,200]{1,0} custom-call(f32[2048,200]{1,0} %maximum.35, f32[200,200]{1,0} %get-tuple-element.41, f32[2048,200]{1,0} %broadcast.262), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/word_model/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.819796: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.19 = f32[2048,400]{1,0} custom-call(f32[2048,400]{1,0} %add.181, f32[400,400]{1,0} %get-tuple-element.31, f32[2048,400]{1,0} %broadcast.272), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/par_model/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.899021: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.27 = f32[2048,500]{1,0} custom-call(f32[2048,927]{1,0} %add.111, f32[927,500]{1,0} %get-tuple-element.15, f32[2048,500]{1,0} %broadcast.294), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/Dense_0/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=25}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.974909: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.31 = f32[2048,500]{1,0} custom-call(f32[2048,500]{1,0} %maximum.31, f32[500,500]{1,0} %get-tuple-element.17, f32[2048,500]{1,0} %broadcast.300), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/Dense_1/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=31}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.988255: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.39 = f32[2048,78]{1,0} custom-call(f32[2048,500]{1,0} %divide.106, f32[500,78]{1,0} %get-tuple-element.21, f32[2048,78]{1,0} %broadcast.312), custom_call_target=\"__cublas$gemm\", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name=\"jit(train_step)/jit(main)/jvp(jit(mse))/vmap(MainModel)/Dense_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=36}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":1,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.990564: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.41 = f32[2048,500]{1,0} custom-call(f32[2048,78]{1,0} %add.133, f32[500,78]{1,0} %get-tuple-element.21), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_3/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=36}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:23.994611: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.43 = f32[2048,500]{1,0} custom-call(f32[2048,500]{1,0} %multiply.330, f32[500,500]{1,0} %get-tuple-element.19), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_2/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=34}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.000433: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.47 = f32[2048,927]{1,0} custom-call(f32[2048,500]{1,0} %select.1, f32[927,500]{1,0} %get-tuple-element.15), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=25}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.002996: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.49 = f32[2048,300]{1,0} custom-call(f32[2048,300]{1,0} %select.2, f32[300,300]{1,0} %get-tuple-element.27), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/char_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.008182: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.51 = f32[2048,960]{1,0} custom-call(f32[2048,300]{1,0} %select.3, f32[960,300]{1,0} %get-tuple-element.25), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/char_model/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.011581: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.53 = f32[2048,400]{1,0} custom-call(f32[2048,400]{1,0} %select.4, f32[400,400]{1,0} %get-tuple-element.33), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/par_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.013388: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.57 = f32[2048,200]{1,0} custom-call(f32[2048,200]{1,0} %select.6, f32[200,200]{1,0} %get-tuple-element.41), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/word_model/Dense_1/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.015237: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.59 = f32[2048,201]{1,0} custom-call(f32[2048,200]{1,0} %select.7, f32[201,200]{1,0} %get-tuple-element.39), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/word_model/Dense_0/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.021256: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.61 = f32[927,500]{1,0} custom-call(f32[2048,927]{1,0} %add.111, f32[2048,500]{1,0} %select.1), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=25}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.025585: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.63 = f32[500,500]{1,0} custom-call(f32[2048,500]{1,0} %maximum.31, f32[2048,500]{1,0} %select.0), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=31}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.028228: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.67 = f32[500,78]{1,0} custom-call(f32[2048,500]{1,0} %divide.106, f32[2048,78]{1,0} %add.133), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/Dense_3/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=36}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.033591: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.69 = f32[960,300]{1,0} custom-call(f32[2048,960]{1,0} %add.155, f32[2048,300]{1,0} %select.3), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/char_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.036351: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.71 = f32[300,300]{1,0} custom-call(f32[2048,300]{1,0} %maximum.33, f32[2048,300]{1,0} %select.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/char_model/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.039883: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.73 = f32[400,400]{1,0} custom-call(f32[2048,400]{1,0} %add.181, f32[2048,400]{1,0} %select.5), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/par_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.041769: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.77 = f32[201,200]{1,0} custom-call(f32[2048,201]{1,0} %add.215, f32[2048,200]{1,0} %select.7), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/word_model/Dense_0/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:49:24.043694: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.79 = f32[200,200]{1,0} custom-call(f32[2048,200]{1,0} %maximum.35, f32[2048,200]{1,0} %select.6), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(train_step)/jit(main)/transpose(jvp(jit(mse)))/vmap(MainModel)/word_model/Dense_1/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] = 0.4606361389160156\n",
      "Loss[1] = 0.4230753779411316\n",
      "Loss[2] = 0.3905617892742157\n",
      "Loss[3] = 0.3623582720756531\n",
      "Loss[4] = 0.34017038345336914\n",
      "Loss[5] = 0.3221537470817566\n",
      "Loss[6] = 0.30464524030685425\n",
      "Loss[7] = 0.289582759141922\n",
      "Loss[8] = 0.27190962433815\n",
      "Loss[9] = 0.2607208788394928\n",
      "Loss[10] = 0.24164626002311707\n",
      "Loss[11] = 0.23695455491542816\n",
      "Loss[12] = 0.2317463755607605\n",
      "Loss[13] = 0.2085336297750473\n",
      "Loss[14] = 0.2087036818265915\n",
      "Loss[15] = 0.19873274862766266\n",
      "Loss[16] = 0.1992875337600708\n",
      "Loss[17] = 0.17953254282474518\n",
      "Loss[18] = 0.1803821325302124\n",
      "Loss[19] = 0.16772569715976715\n",
      "process took 0:03:17.498049 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Minimise the loss.\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "# Create optimiser state.\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "for step in range(20):\n",
    "    \n",
    "    rng, sample_rng = jax.random.split(rng)\n",
    "        \n",
    "        \n",
    "    train_data_loader = train_data_collator(\n",
    "        sample_rng,\n",
    "        pd.DataFrame(X_train_char).to_numpy(),\n",
    "        pd.DataFrame(X_train_word).to_numpy(),\n",
    "        pd.DataFrame(X_train_par).to_numpy(),\n",
    "        pd.DataFrame(X_train_rest).to_numpy(),\n",
    "        onehot(y_train_int, num_labels),#dst_y,\n",
    "        2048\n",
    "    )\n",
    "    \n",
    "    for batch in train_data_loader:\n",
    "        \n",
    "        # # Compute gradient of the loss.\n",
    "        # loss_val, grads = loss_grad_fn(params, batch)\n",
    "        # # Update the optimiser state, create an update to the params.\n",
    "        # updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        # # Update the parameters.\n",
    "        # params = optax.apply_updates(params, updates)\n",
    "        params, opt_state, loss_val = train_step_fn(\n",
    "            params, opt_state, batch)\n",
    "        \n",
    "        # state, loss_val = train_step_fn(state, batch)\n",
    "     \n",
    "    print(f'Loss[{step}] = {loss_val}')\n",
    "        \n",
    "print(f'process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c16c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03392047 -0.00695269 -0.00603585 ... -0.0209151   0.02655698\n",
      "  -0.01436193]\n",
      " [ 0.01360337 -0.01475731 -0.0469784  ...  0.03361696 -0.0330577\n",
      "   0.01963609]\n",
      " [ 0.01943858  0.06209303  0.01180282 ...  0.01107159 -0.00382269\n",
      "  -0.05611707]\n",
      " ...\n",
      " [-0.08034991 -0.02015796  0.05920383 ...  0.03895368  0.10644104\n",
      "   0.08387353]\n",
      " [-0.00046567  0.01686141 -0.05507798 ...  0.0267499  -0.01370377\n",
      "   0.05393811]\n",
      " [ 0.03199979  0.02315453  0.0341112  ...  0.05676383  0.0363528\n",
      "   0.04768129]]\n",
      "(927, 500)\n"
     ]
    }
   ],
   "source": [
    "#check param\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"])\n",
    "print(params[\"params\"][\"Dense_0\"][\"kernel\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1808d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "331f3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_v_1 = jnp.array(pd.DataFrame(X_val_char).to_numpy())\n",
    "j_v_2 = jnp.array(pd.DataFrame(X_val_word).to_numpy())\n",
    "j_v_3 = jnp.array(pd.DataFrame(X_val_par).to_numpy())\n",
    "j_v_4 = jnp.array(pd.DataFrame(X_val_rest).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cfc34f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 10:52:39.502740: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,300]{1,0} custom-call(f32[137353,960]{1,0} %Arg_0.1, f32[960,300]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:39.833944: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,300]{1,0} custom-call(f32[137353,300]{1,0} %Arg_0.1, f32[300,300]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:40.178888: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,200]{1,0} custom-call(f32[137353,201]{1,0} %Arg_0.1, f32[201,200]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:40.477976: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,200]{1,0} custom-call(f32[137353,200]{1,0} %Arg_0.1, f32[200,200]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=19}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:40.928494: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,400]{1,0} custom-call(f32[137353,400]{1,0} %Arg_0.1, f32[400,400]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2757493026.py\" source_line=14}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:41.934374: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,500]{1,0} custom-call(f32[137353,927]{1,0} %Arg_0.1, f32[927,500]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=25}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "2023-03-21 10:52:42.348904: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,500]{1,0} custom-call(f32[137353,500]{1,0} %Arg_0.1, f32[500,500]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=31}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137353, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 10:52:43.045989: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:219] Failed to find best cuBLAS algorithm, GEMM performance might be suboptimal: INTERNAL: All algorithms tried for %cublas-gemm.1 = f32[137353,78]{1,0} custom-call(f32[137353,500]{1,0} %Arg_0.1, f32[500,78]{1,0} %Arg_1.2), custom_call_target=\"__cublas$gemm\", metadata={op_name=\"jit(dot_general)/jit(main)/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1299052/2167800792.py\" source_line=36}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# print(dst_x.shape) #(412059, 960)\n",
    "y_pred = mainmodel.apply(params, j_v_1, j_v_2, j_v_3, j_v_4)\n",
    "print(y_pred.shape) #(137353, 78)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacc61d",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d93b8673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['county' 'credit' 'age' ... 'duration' 'class' 'jockey']\n",
      "(137353,)\n",
      "0.6919616414923909\n",
      "0.7329071807678027\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_pred_classes = helpers._proba_to_classes(y_pred, \"sherlock\")\n",
    "\n",
    "print(y_pred_classes)\n",
    "print(y_pred_classes.shape)\n",
    "\n",
    "print(f1_score(y_validation, y_pred_classes, average=\"weighted\"))\n",
    "print(accuracy_score(y_validation, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherlock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
