{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Environment does not have key: PYTHONHASHSEED\n"
     ]
    }
   ],
   "source": [
    "# If you need fully deterministic results between runs, set the following environment value prior to launching jupyter.\n",
    "# See comment in sherlock.features.paragraph_vectors.infer_paragraph_embeddings_features for more info.\n",
    "%env PYTHONHASHSEED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below first downloads the data (roughly 700K samples), then extract features from the raw data values. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pyfunctional==1.4.3\n",
    "-e .\n",
    "gdown==4.3.0\n",
    "nltk==3.4.5\n",
    "gensim==3.8.0\n",
    "'''\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --user numpy\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.functional import extract_features_to_csv\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model, initialise_nltk\n",
    "from sherlock.features.preprocessing import (\n",
    "    extract_features,\n",
    "    convert_string_lists_to_lists,\n",
    "    prepare_feature_extraction,\n",
    "    load_parquet_values,\n",
    ")\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2023-03-20 17:29:08.849349.\n"
     ]
    }
   ],
   "source": [
    "print(f'Started at {datetime.now()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "This will download the raw values and preprocessed files, the corresponding labels as well as a few other supporting files:\n",
    "- `download_data()` will download 3.6GB of data for preprocessing and model training into the `data/` directory.\n",
    "- `prepare_feature_extraction()` will download +/- 800 MB of data into the `features/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n"
     ]
    }
   ],
   "source": [
    "# helpers.download_data()\n",
    "prepare_feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy'):\n",
    "    raise SystemExit(\n",
    "        \"\"\"\n",
    "        Trained paragraph vectors do not exist,\n",
    "        please run the '03-retrain-paragraph-vector-features' notebook before continuing\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in raw data\n",
    "You can skip this step if you want to use a preprocessed data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report memory usage (can be slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "The input data is assumed to be a dataframe of column values stored as stringed lists \"['hello', 'goodbye', 'hi']\", stored in a parquet file.\n",
    "\n",
    "`TODO`: ideally we pickle the dataframe with value lists to avoid parsing these strings into lists again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET FILE PATHS\n",
    "\n",
    "X_filename_csv = f'../data/data/processed/processedX.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 4 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt, \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy,\n",
      "        \n",
      " ../sherlock/features/par_vec_trained_400.pkl.trainables.syn1neg.npy, and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.wv.vectors.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:04.878021 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:00.005301 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n",
      "Initialised NLTK, process took 0:00:00.320742 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sunnykim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sunnykim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)\n",
    "initialise_nltk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv -> pq -> csv -> pq \n",
    "\n",
    "df = pd.read_csv(\"../data/data/raw/inputX.csv\", index_col = [0])\n",
    "df.to_parquet(\"../data/data/raw/inputX_pq.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/data/processed/processedX.csv at 2023-03-20 16:44:17.120967. Rows=4, using 32 CPU cores\n",
      "Exporting 1588 column features\n",
      "Finished. Processed 4 rows in 0:00:00.961981, key_count=1\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/data/raw/inputX_pq.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Locally Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (test) process took 0:00:00.128780 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "X_test = pd.read_csv(X_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>-0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.33795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.742677</td>\n",
       "      <td>1.326868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>-0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>-0.001041</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.001213</td>\n",
       "      <td>-0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>-0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1588 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0            0.0            0.0        0.000000        0.00000            0.0   \n",
       "1            1.0            0.0        0.368421        0.33795            0.0   \n",
       "2            0.0            0.0        0.000000        0.00000            0.0   \n",
       "3            0.0            0.0        0.000000        0.00000            0.0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0            0.0               0.0            0.0           -3.000000   \n",
       "1            2.0               0.0            7.0            0.742677   \n",
       "2            0.0               0.0            0.0           -3.000000   \n",
       "3            0.0               0.0            0.0           -3.000000   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0            0.000000  ...    -0.000260     0.000504     0.001107   \n",
       "1            1.326868  ...     0.000176     0.000030    -0.000322   \n",
       "2            0.000000  ...     0.000085    -0.001041     0.000391   \n",
       "3            0.000000  ...    -0.000839     0.000855     0.000757   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0     0.000043     0.000379     0.000412     0.000205    -0.000171   \n",
       "1     0.000328    -0.000179     0.000946     0.000128     0.000599   \n",
       "2     0.000069     0.000707     0.000756    -0.000561    -0.000595   \n",
       "3    -0.000059    -0.000402     0.000039    -0.000081    -0.000691   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000661    -0.000717  \n",
       "1     0.000912    -0.000503  \n",
       "2    -0.001213    -0.000172  \n",
       "3    -0.000319    -0.000900  \n",
       "\n",
       "[4 rows x 1588 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN values with feature means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose process took 0:00:00.002447 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_columns_means = pd.DataFrame(X_test.mean()).transpose()\n",
    "\n",
    "print(f'Transpose process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.fillna(train_columns_means.iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_parquet('../data/data/processed/inputX.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y data to parquet\n",
    "df = None\n",
    "df = pd.read_csv(\"../data/data/raw/inputY.csv\", index_col=[0])\n",
    "df.to_parquet(\"../data/data/raw/inputY_pq.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sherlock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e0131c35a37b1ea9c8fba24a01fa702cfdaebeacafed5d319d35dfa8098ea7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
